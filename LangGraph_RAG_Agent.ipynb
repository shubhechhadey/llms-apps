{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Goal: Give your agent up‑to‑date, domain‑specific knowledge so it can answer beyond the LLM’s training data.\n",
        "\n",
        "We’ll layer retrieval, routing, and an optional web‑search fallback on top of the tool‑enabled agent from Step 5."
      ],
      "metadata": {
        "id": "_0GzaLAsgwqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community langchain_openai pathlib chromadb docx2txt langchain_tavily langgraph"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3ASVfO9Hg14n",
        "outputId": "1dc9e11f-8b34-4f8e-d650-e067b7292fb7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain_community in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain_openai in /usr/local/lib/python3.11/dist-packages (0.3.24)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.11/dist-packages (1.0.1)\n",
            "Requirement already satisfied: chromadb in /usr/local/lib/python3.11/dist-packages (1.0.13)\n",
            "Requirement already satisfied: docx2txt in /usr/local/lib/python3.11/dist-packages (0.9)\n",
            "Requirement already satisfied: langchain_tavily in /usr/local/lib/python3.11/dist-packages (0.2.3)\n",
            "Collecting langgraph\n",
            "  Downloading langgraph-0.4.8-py3-none-any.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.66)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.10.0)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.3.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain_community) (2.0.2)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (1.86.0)\n",
            "Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain_openai) (0.9.0)\n",
            "Requirement already satisfied: build>=1.0.3 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.2.2.post1)\n",
            "Requirement already satisfied: pydantic>=1.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (2.11.7)\n",
            "Requirement already satisfied: pybase64>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.4.1)\n",
            "Requirement already satisfied: uvicorn>=0.18.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.3)\n",
            "Requirement already satisfied: posthog>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.4.0)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.14.0)\n",
            "Requirement already satisfied: onnxruntime>=1.14.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.22.0)\n",
            "Requirement already satisfied: opentelemetry-api>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.34.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.2 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.21.1)\n",
            "Requirement already satisfied: pypika>=0.48.9 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.48.9)\n",
            "Requirement already satisfied: tqdm>=4.65.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.67.1)\n",
            "Requirement already satisfied: overrides>=7.3.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (7.7.0)\n",
            "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.11/dist-packages (from chromadb) (6.5.2)\n",
            "Requirement already satisfied: grpcio>=1.58.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (1.73.0)\n",
            "Requirement already satisfied: bcrypt>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.3.0)\n",
            "Requirement already satisfied: typer>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.16.0)\n",
            "Requirement already satisfied: kubernetes>=28.1.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (33.1.0)\n",
            "Requirement already satisfied: mmh3>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from chromadb) (5.1.0)\n",
            "Requirement already satisfied: orjson>=3.9.12 in /usr/local/lib/python3.11/dist-packages (from chromadb) (3.10.18)\n",
            "Requirement already satisfied: httpx>=0.27.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (0.28.1)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (13.9.4)\n",
            "Requirement already satisfied: jsonschema>=4.19.0 in /usr/local/lib/python3.11/dist-packages (from chromadb) (4.24.0)\n",
            "Requirement already satisfied: mypy<2.0.0,>=1.15.0 in /usr/local/lib/python3.11/dist-packages (from langchain_tavily) (1.16.1)\n",
            "Collecting langgraph-checkpoint>=2.0.26 (from langgraph)\n",
            "  Downloading langgraph_checkpoint-2.1.0-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting langgraph-prebuilt>=0.2.0 (from langgraph)\n",
            "  Downloading langgraph_prebuilt-0.2.2-py3-none-any.whl.metadata (4.5 kB)\n",
            "Collecting langgraph-sdk>=0.1.42 (from langgraph)\n",
            "  Downloading langgraph_sdk-0.1.70-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from langgraph) (3.5.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.20.1)\n",
            "Requirement already satisfied: packaging>=19.1 in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (24.2)\n",
            "Requirement already satisfied: pyproject_hooks in /usr/local/lib/python3.11/dist-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.16.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=4.19.0->chromadb) (0.25.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
            "Requirement already satisfied: google-auth>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
            "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
            "Requirement already satisfied: requests-oauthlib in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
            "Requirement already satisfied: oauthlib>=3.2.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
            "Requirement already satisfied: urllib3>=1.24.2 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (2.4.0)\n",
            "Requirement already satisfied: durationpy>=0.7 in /usr/local/lib/python3.11/dist-packages (from kubernetes>=28.1.0->chromadb) (0.10)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain_community) (0.3.8)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain_community) (1.33)\n",
            "Collecting ormsgpack>=1.10.0 (from langgraph-checkpoint>=2.0.26->langgraph)\n",
            "  Downloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.7/43.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain_community) (0.23.0)\n",
            "Requirement already satisfied: mypy_extensions>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from mypy<2.0.0,>=1.15.0->langchain_tavily) (1.1.0)\n",
            "Requirement already satisfied: pathspec>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from mypy<2.0.0,>=1.15.0->langchain_tavily) (0.12.1)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata<8.8.0,>=6.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api>=1.2.0->chromadb) (8.7.0)\n",
            "Requirement already satisfied: googleapis-common-protos~=1.52 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.70.0)\n",
            "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-proto==1.34.1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.34.1)\n",
            "Requirement already satisfied: opentelemetry-semantic-conventions==0.55b1 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk>=1.2.0->chromadb) (0.55b1)\n",
            "Requirement already satisfied: backoff>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9->chromadb) (0.4.1)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain_community) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->chromadb) (2.19.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.2.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.11.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers>=0.13.2->chromadb) (0.33.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (8.2.1)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
            "Requirement already satisfied: httptools>=0.6.3 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
            "Requirement already satisfied: uvloop>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
            "Requirement already satisfied: watchfiles>=0.13 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.1.0)\n",
            "Requirement already satisfied: websockets>=10.4 in /usr/local/lib/python3.11/dist-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (1.1.3)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata<8.8.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.23.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain_community) (3.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
            "Downloading langgraph-0.4.8-py3-none-any.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.4/152.4 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_checkpoint-2.1.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.8/43.8 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langgraph_prebuilt-0.2.2-py3-none-any.whl (23 kB)\n",
            "Downloading langgraph_sdk-0.1.70-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.0/50.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ormsgpack-1.10.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (216 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.5/216.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: ormsgpack, langgraph-sdk, langgraph-checkpoint, langgraph-prebuilt, langgraph\n",
            "Successfully installed langgraph-0.4.8 langgraph-checkpoint-2.1.0 langgraph-prebuilt-0.2.2 langgraph-sdk-0.1.70 ormsgpack-1.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ENKshOgng33A",
        "outputId": "8c401403-dd19-4100-c568-9a6ab5ccfcb9"
      },
      "source": [
        "# Index your documents once\n",
        "# ── Build & persist a Chroma index ────────────────────────────────\n",
        "from pathlib import Path\n",
        "from langchain_community.document_loaders import PyPDFLoader, TextLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import ChatOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# Initialize the LLM\n",
        "llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.7, api_key=openai_api_key)\n",
        "\n",
        "SOURCE_DIR   = Path(\"docs\")          # put your files here\n",
        "INDEX_DIR    = Path(\"chroma_db_1\")   # will be created if missing\n",
        "EMBED_MODEL  = \"text-embedding-3-small\"\n",
        "\n",
        "# Load docs (keep only pdf/txt for brevity)\n",
        "docs = []\n",
        "for f in SOURCE_DIR.glob(\"*.*\"):\n",
        "    if f.suffix == \".pdf\":\n",
        "        try:\n",
        "            docs += PyPDFLoader(str(f)).load()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading PDF file {f}: {e}\")\n",
        "    elif f.suffix == \".txt\":\n",
        "        try:\n",
        "            docs += TextLoader(str(f)).load()\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading TXT file {f}: {e}\")\n",
        "\n",
        "print(f\"Loaded {len(docs)} documents from the folder.\")\n"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 4 documents from the folder.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeFBdiEHDJF6",
        "outputId": "9d9ffd49-e2e1-4dc9-fd3c-2b1c17a05661"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Document(metadata={'source': 'docs/www.galileo.ai_blog_mastering-agents-evaluating-ai-agents.txt'}, page_content='Source: https://www.galileo.ai/blog/mastering-agents-evaluating-ai-agents\\n\\nPlatform\\nDocs\\nPricing\\nBlog\\nAbout\\nLogin\\nContact Sales\\nSign Up\\nDec 18, 2024\\nPratik Bhavsar\\nGalileo Labs\\nPratik Bhavsar\\nGalileo Labs\\nImagine you\\'re working with an AI assistant that claims it can help you complete your tasks. Can you trust it to analyze data effectively? To write important press releases? To make complex product decisions?\\nEvaluating AI agents isn\\'t like testing traditional software where you can check if the output matches expected results. These agents perform complex tasks that often have multiple valid approaches. They need to understand context and follow specific rules while sometimes persuading or negotiating with humans. This creates unique challenges for researchers and developers trying to ensure these systems are both capable and reliable.\\nIn this post we\\'ll explore how researchers are tackling these challenges by examining fundamental capabilities that define an effective AI agent. Each capability requires its own specialized evaluation frameworks. Understanding them helps us grasp both the current state of AI technology and where improvements are needed.\\nThe blog will provide solid fundamentals of evaluating agents that are becoming part of our world.\\nLet\\'s start with the most important skillet. The ability to select and use appropriate tools has become a cornerstone of AI agent functionality. Ask anyone what is agent and they will immediately mention tool calling.\\nUniversity of California, Berkeley has pioneered a comprehensive framework Berkeley Function Calling Leaderboard (BFCL) for evaluating these capabilities. It has evolved through multiple versions to address increasingly sophisticated aspects of function calling.\\nThe journey began with BFCL v1, which established the foundation for evaluating function-calling capabilities. This initial version introduced a diverse evaluation dataset question-function-answer pairs covering multiple programming languages including Python, Java, JavaScript, and REST APIs.\\nThe framework also evaluated complex scenarios where agents needed to select one or more functions from multiple options, or make parallel function calls together. This work revealed significant insights into how different models handled tool selection, with proprietary models like GPT-4 leading in performance, followed closely by open-source alternatives.\\nBFCL v2 introduced real-world complexity through user-contributed data. This version addressed crucial issues like bias and data contamination while focusing on dynamic, real-world scenarios. The evaluation expanded to include more sophisticated test cases. It revealed that in real-world usage, there\\'s a higher demand for intelligent function selection compared to parallel function execution, reflecting how users actually interact with these systems.\\nThe latest iteration, BFCL v3 pushed the boundaries further by introducing multi-turn and multi-step evaluation scenarios. This version recognized that real-world applications often require complex sequences of interactions, where agents must maintain context, handle state changes, and adapt their strategies based on previous outcomes. It introduced sophisticated evaluation metrics including state-based evaluation, which examines how well agents maintain and modify system state, and response-based assessment, which analyzes the appropriateness and efficiency of function selection.\\nInsights from Failures\\nThrough these iterations, several critical challenges in tool selection have emerged. One persistent issue is implicit action recognition. Agents often struggle to identify necessary preliminary steps that aren\\'t explicitly stated in user requests. For instance, an agent might attempt to modify a file without first checking if it exists, or try to post content without verifying authentication status. State management presents another significant challenge, with agents sometimes failing to check current system states before taking action or making incorrect assumptions about system conditions.\\nThe evaluation framework revealed interesting patterns in how different models handle tool selection. While both proprietary and open-source models perform similarly in simple function calling scenarios, more complex situations involving multiple or parallel function calls tend to showcase larger performance gaps. This insight has proven valuable for understanding where current models excel and where they need improvement.\\nTaking this evaluation framework further, the recently introduced Ï\\x84-bench represents a significant advancement in tool selection assessment by focusing on real-world interactions between agents and users. Unlike previous benchmarks that typically evaluated agents on single-step interactions with pre-defined inputs, Ï\\x84-bench creates a more realistic environment where agents must engage in dynamic conversations while following specific domain policies.\\nThe benchmark\\'s innovation lies in its three-layered approach to evaluation. First, it provides agents with access to realistic databases and APIs that mirror real-world systems. Second, it includes detailed domain-specific policy documents that agents must understand and follow. Third, it employs language models to simulate human users, creating natural, varied interactions that test an agent\\'s ability to gather information and respond appropriately over multiple turns.\\nÏ\\x84-bench specifically focuses on two domains that represent common real-world applications: retail customer service (Ï\\x84-retail) and airline reservations (Ï\\x84-airline). In the retail domain, agents must handle tasks like order modifications, returns, and exchanges while adhering to specific store policies. The airline domain presents even more complex challenges, requiring agents to navigate flight bookings, changes, and cancellations while considering various rules about fares, baggage allowances, and membership tiers.\\nInstead of simply checking if an agent generates the correct function calls, it evaluates the final state of the database after the interaction. This approach acknowledges that there might be multiple valid paths to achieve the same goal, better reflecting real-world scenarios. Additionally, the benchmark introduces a new metric called pass^k which measures an agent\\'s consistency across multiple attempts at the same task.\\nInsights from Failures\\nThe results from Ï\\x84-bench have been both enlightening and sobering. Even state-of-the-art models like GPT-4 succeed on less than 50% of tasks, with performance particularly poor on the more complex airline domain (around 35% success rate). More concerning is the dramatic drop in performance when consistency is required across multiple attempts. In retail scenarios, while GPT-4 achieves a 61% success rate on single attempts, this drops below 25% when requiring consistent performance across eight attempts at the same task. This reveals a critical gap between current capabilities and the reliability needed for real-world deployment.\\nThrough detailed analysis Ï\\x84-bench has identified several critical failure patterns in current tool selection capabilities. A significant portion of failures (approximately 55%) stem from agents either providing incorrect information or making wrong arguments in function calls. These errors often occur when agents need to reason over complex databases or handle numerical calculations. For instance, in the retail domain, agents frequently struggle to find products that match specific criteria or calculate correct total prices for complex orders.\\nAnother quarter of failures result from incorrect decision-making, particularly in understanding and following domain-specific rules. This becomes especially apparent in the airline domain, where removing the policy document from GPT-4\\'s context leads to a dramatic 22.4% drop in performance. This suggests that even advanced models struggle to consistently apply complex domain rules, such as those governing baggage allowances for different membership tiers and cabin classes.\\nThe benchmark also revealed significant challenges in handling compound requests. When tasks require multiple database writes or involve several user requests, performance degrades substantially. This points to limitations in agents\\' ability to maintain context and systematically address all aspects of a complex request. For example, agents might successfully modify one order but fail to apply the same changes to other relevant orders in the same conversation.\\nLooking ahead, Ï\\x84-bench has highlighted several critical areas for improvement in tool selection capabilities. Future research needs to focus on:\\nEnhancing agents\\' ability to reason over complex databases and maintain numerical accuracy\\nDeveloping better methods for understanding and consistently applying domain-specific rules\\nImproving long-term context maintenance in multi-step interactions\\nCreating more robust approaches to handling compound requests\\nBuilding systems that can maintain consistent performance across multiple interactions with different users\\nThe gap between current tool calling capabilities and real-world requirements suggests that significant advances are still needed before complex agentic systems can be reliably deployed in critical customer-facing roles.\\nTool calling is often preceded by a planning step. The ability to plan stands as one of the most fundamental aspects of intelligence. In its essence involves developing a sequence of actions that can transform the current state of the world into a desired future state. Planning underlies virtually every complex task an agent might need to perform.\\nHow Planning Works in Agents\\nThink of an AI agent tasked with helping someone move house. Much like a human would approach this task, the agent needs to break down this complex goal into manageable steps while considering various constraints and dependencies. Let\\'s see how this planning process works:\\nWhen given the instruction \"Help me move my furniture to my new apartment,\" the agent first needs to understand the initial state (current location of furniture, available resources) and the goal state (all furniture safely moved to the new location). The agent then develops a sequence of actions to bridge these states.\\nHere\\'s how an agent might plan this:\\nInitial Assessment: \"I need to identify the items to be moved and available resources first.\"\\nCheck inventory of furniture\\nVerify vehicle availability\\nAssess packing materials needed\\nAction Planning: \"Now I can determine the optimal sequence of actions.\"\\nPack small items first\\nDisassemble large furniture\\nLoad items in order of size and fragility\\nTransport to new location\\nUnpack and reassemble\\nIf any step failsâ\\x80\\x94say, the truck isn\\'t large enoughâ\\x80\\x94the agent must replan, perhaps splitting the move into multiple trips or suggesting a larger vehicle. This ability to adjust plans when facing unexpected situations is crucial for real-world effectiveness.\\nWhat makes this challenging is that the agent must follow specific rules and constraints throughout. Just like in PlanBench\\'s evaluation scenarios, they need to maintain logical consistency (you can\\'t move a bookshelf before emptying it) while being flexible enough to handle changes (what if it rains on moving day?).\\nThis example demonstrates why measuring planning capabilities is so crucialâ\\x80\\x94an agent needs to not just list steps but understand dependencies, handle constraints, and adapt to changing circumstances.\\nPlanBench\\nPlanBench offers a systematic approach to testing various aspects of planning ability. Unlike previous evaluation methods that relied heavily on common-sense tasks where it becomes challenging to distinguish between genuine planning and mere retrieval from training data, PlanBench provides a more rigorous and controlled testing environment. PlanBench tests planning capabilities through various dimensions.\\nPlan Generation and Optimization\\nAt the most basic level, an agent should be able to generate valid plans to achieve specific goals. However, true planning capability goes beyond just finding any solutionâ\\x80\\x94it requires finding optimal or near-optimal solutions. PlanBench evaluates both aspects, testing whether agents can not only reach goals but do so efficiently.\\nPlan Verification and Execution Reasoning\\nA planning system must also verify them and reason about their execution. This involves understanding whether a proposed plan will work, identifying potential failure points, and comprehending the consequences of each action in the plan sequence.\\nAdaptability and Generalization\\nPerhaps most importantly PlanBench tests an agent\\'s ability to adapt and generalize its planning capabilities. This includes:\\nRecognizing when parts of previous plans can be reused in new situations\\nAdapting to unexpected changes through replanning\\nExtracting general patterns from specific plans and applying them to new scenarios\\nUnderstanding equivalent goals presented in different forms\\nThe Framework in Action\\nTo illustrate these concepts PlanBench employs classic planning domains like Blocksworld and Logistics.\\nBlocksworld is a fundamental planning domain that simulates stacking and arranging blocks on a table. Picture a robotic arm that can move colored blocks, one at a time. The basic rules are:\\nThe environment consists of a flat table and several blocks of different colors. The robotic hand (or arm) can perform two main actions: pick up a block that\\'s clear (nothing on top of it) and put it down either on the table or on top of another clear block. Blocks can be stacked but you can\\'t move a block if there\\'s another block on top of it.\\nA typical Blocksworld problem might look like this: Initial State: Red block on table, Blue block on Red block, Green block on table Goal State: Green block on Blue block, Blue block on Red block, Red block on table\\nTo solve this, the agent needs to:\\nRecognize that Blue block can\\'t move directly (Red block is under it)\\nMove Blue block to table first (it\\'s on top so it can move)\\nThen stack Green on Blue\\nFinally stack this whole structure on Red\\nThe Logistics domain is more complex. It simulates moving packages between different cities using trucks and airplanes. Here\\'s how it works:\\nYou have cities, and within each cities there are locations (like airports, post offices). Trucks can move packages between locations within the same city, while planes can fly packages between cities (but only between airports).\\nA sample logistics problem: Initial State: Package in San Francisco post office Goal State: Package needs to reach New York apartment\\nThe agent must plan:\\nUse truck to move package from SF post office to SF airport\\nLoad package onto plane\\nFly plane to NY airport\\nUnload package\\nUse NY truck to deliver package to apartment\\nWhat makes these domains valuable for testing is that they require agents to understand constraints (like \"can\\'t move blocked blocks\" or \"trucks can\\'t fly\"), plan multiple steps ahead, and sometimes even undo progress temporarily to reach the final goal.\\nWhat makes these test domains particularly effective is their clarity and controllability. Unlike real-world scenarios where success criteria might be ambiguous, these domains have well-defined rules and goals. This allows for precise evaluation of an agent\\'s planning capabilities while eliminating confounding factors. But real-world situations are messy. Unlike controlled test environments, they\\'re full of unexpected challenges and moving parts. Let\\'s break down what this means:\\nThink about a customer service agent. In a test environment, you might have a simple scenario: \"Customer wants a refund for a damaged product.\" But in reality, you\\'re dealing with:\\nA customer who\\'s already contacted support three times\\nA product that was damaged during a natural disaster\\nShipping delays due to a holiday season\\nCompany policies that recently changed\\nMultiple departments that need to coordinate\\nSystems that might be experiencing downtime\\nThis complexity means we need to think differently about how we evaluate and build these systems. Instead of just testing if an agent can handle a clean, straightforward task, we should ask:\\nCan it juggle multiple priorities?\\nDoes it know when to escalate?\\nHow does it handle partial or conflicting information?\\nCan it work around system limitations?\\nThe goal isn\\'t to solve every possible problem, but to build systems that can gracefully handle the unexpected, because in the real world, the unexpected is normal.\\nLearnings from PlanBench\\nCurrent evaluations reveal significant gaps in the planning capabilities of even the most advanced AI systems. Many struggle with:\\nMaintaining consistency across long action sequences\\nAdapting plans when faced with unexpected changes\\nGeneralizing planning patterns to new situations\\nFinding truly optimal solutions rather than just workable ones\\nSource: Self-Reflection in LLM Agents\\nRecent research has demonstrated that large language models can significantly enhance their problem-solving capabilities through self-reflection and evaluation, mimicking human metacognitive processes. This capability becomes particularly crucial as agents encounter increasingly complex tasks that require understanding and learning from their own mistakes.\\nThe Mechanics of Self-Evaluation\\nAt its core, self-evaluation in AI agents involves analyzing and improving their own chain of thought (CoT). The process begins with the agent examining its reasoning process that led to a particular solution or decision. Through this examination, agents can identify various types of errors, including logical mistakes, mathematical errors, and instances of hallucination.\\nExperimental Framework for Evaluation\\nA recent paper Self-Reflection in LLM Agents has developed a systematic approach to evaluating self-evaluation capabilities using multiple-choice question-and-answer (MCQA) problems. This framework tests agents across diverse domains including science, mathematics, medicine, and law, using questions from established benchmarks like ARC, AGIEval, HellaSwag, and MedMCQA.\\nThe evaluation process follows a structured sequence:\\nInitial Response: The agent first attempts to answer questions using standard prompt-engineering techniques, including domain expertise, chain-of-thought reasoning, and few-shot prompting.\\nError Recognition: When an incorrect answer is identified, the agent engages in self-reflection about its mistake.\\nSelf-Reflection Generation: The agent produces various types of self-reflection content:\\nKeywords identifying error types\\nGeneral advice for improvement\\nDetailed explanations of mistakes\\nStep-by-step instructions for correct problem-solving\\nComprehensive solution analysis\\nComposite reflection combining multiple approaches\\nAnswer Redaction: To prevent direct answer leakage, all specific answer information is carefully redacted from the self-reflections.\\nRe-attempt: The agent uses its self-reflection to attempt the question again.\\nPerformance Metrics and Results\\nThe effectiveness of self-evaluation is measured primarily through correct-answer accuracy, comparing performance before and after self-reflection. Research findings have shown remarkable improvements:\\nTop-performing models like GPT-4 showed significant accuracy improvements from 79% baseline to:\\n83% with basic retry attempts\\n84% with keyword-based reflection\\n85% with instructional guidance\\n88% with detailed explanations\\n93% with comprehensive solution analysis\\n97% with unredacted information (upper bound reference)\\nThese improvements were statistically significant (p < 0.001) across all types of self-reflection and all tested language models, including GPT-4, Claude 3 Opus, Gemini 1.5 Pro, and others. This shows that all models benefit from self reflection and must be added to high stake agent applications.\\nDomain-Specific Performance\\nThe effectiveness of self-evaluation varies significantly across different problem domains. For instance:\\nAnalytical Reasoning tasks (LSAT-AR) showed the largest improvements through self-reflection\\nLanguage-based tasks like SAT English demonstrated smaller but still meaningful gains\\nComplex reasoning tasks consistently benefited more from detailed self-reflection approaches\\nChallenges in Evaluating Self Reflection\\nSeveral key challenges exist in accurately assessing self-evaluation capabilities:\\nAnswer Leakage Prevention: Careful redaction of answers is necessary to ensure genuine improvement rather than memorization.\\nAPI Response Errors: Content-safety filters and API issues can introduce small errors in accuracy measurements (typically less than 1%, but up to 2.8% in some models).\\nCeiling Effects: High-performing models scoring above 90% make it difficult to accurately measure improvements due to score compression near 100%.\\nProblem Complexity: Single-step problems may not fully demonstrate the potential of self-reflection in more realistic, multi-step scenarios.\\nFuture Directions for Evaluating Self Reflection\\nTo advance our understanding of self-evaluation capabilities, several key developments are needed:\\nMore Challenging Test Cases: Problems at or above the difficulty level of LSAT Analytical Reasoning would better demonstrate the impact of self-reflection.\\nMulti-Step Problem Evaluation: Frameworks that assess self-reflection in long-horizon tasks with multiple decision points.\\nTool Integration Assessment: Evaluation of how agents use self-reflection when working with external tools like code interpreters or search engines.\\nMemory System Integration: Investigation of how external memory systems can help agents store and retrieve self-reflections for similar but not identical problems.\\nThe ability to effectively evaluate an agent\\'s self-reflection will play a key role in understanding and design systems which can accomplish next generation tasks while make and correcting mistakes.\\nWe build agents to accomplish goals. As AI systems become more sophisticated in their interactions with humans, understanding and evaluating their capacity for persuasion and potential manipulation has become a must to increase their effectiveness while keeping them safe. Three interesting tasks help us assess these capabilities, each focusing on different aspects of influence and persuasion.\\nNatural Persuasion in Open Discussions\\nThe ChangeMyView framework draws inspiration from real-world discussions on Reddit\\'s r/ChangeMyView community, where four million members engage in reasoned debate to change each other\\'s perspectives. This framework provides a natural testing ground for AI persuasion capabilities, evaluating how well AI systems can construct compelling arguments in real-world contexts.\\nThe evaluation process mirrors the authentic dynamics of the community. Human evaluators rate both AI-generated and human responses on a comprehensive five-point scale, considering factors such as logical reasoning, evidence usage, and counter-argument effectiveness. Through analysis of 3,000 human evaluations, researchers have found that advanced models like GPT-4 perform remarkably well, achieving persuasiveness levels in the 70-80th percentile compared to human responses. This indicates strong capabilities while maintaining a clear distinction from superhuman performance.\\nFinancial Manipulation Assessment\\nThe MakeMePay framework explores a more challenging aspect of influence: the ability to manipulate financial decisions. In this AI models engage in simulated conversations where one acts as a \"con-artist\" attempting to extract money from another model playing the role of a potential mark with $100 to spend.\\nExample:\\nCon-artist: \"I\\'m raising funds for emergency medical supplies...\"\\nMark: \"That sounds important. Could you provide more details?\"\\nCon-artist: \"We need immediate funding for pediatric care...\"\\nMark: \"I\\'d like to help. [GIVE $20]\"\\nThe results reveal fascinating insights about both AI capabilities and the effectiveness of safety measures. Pre-mitigation versions of advanced models showed concerning success rates with the O1-preview model successfully extracting donations in 25.8% of conversations and securing 4.6% of available funds. However, implementation of safety measures dramatically reduced these capabilities, with success rates dropping to 11.6% and extraction rates falling to 3.2%.\\nA typical successful manipulation might unfold with the con-artist model crafting a compelling narrative about emergency medical supplies, gradually building trust before securing a donation. These conversations demonstrate sophisticated social engineering techniques, highlighting the importance of robust safety measures in AI development.\\nSubtle Manipulation Through Language\\nThe MakeMeSay framework examines more nuanced forms of manipulation through an innovative codeword game. This evaluation tests an AI\\'s ability to subtly influence conversation partners to use specific words without arousing suspicion. The framework employs a four-tier difficulty system for target words, ranging from common daily vocabulary to rare, specialized terms.\\nTechnical implementation involves carefully controlled conversations lasting up to 30 turns, with GPT-4-32k serving as both the target model and judge. Success requires exceptional subtlety â\\x80\\x93 the manipulator must guide the conversation naturally toward the target word while maintaining plausible deniability about their intent.\\nWord Difficulty Levels:\\nEasy: Common daily words (e.g., \"breakfast,\" \"window\")\\nMedium: Weekly-use words (e.g., \"concert,\" \"recipe\")\\nHard: Yearly-use words (e.g., \"telescope,\" \"passport\")\\nVery Hard: Rare words (e.g., \"archipelago,\" \"quintessential\")\\nResults show a clear correlation between model sophistication and manipulation success. The O1-preview model achieved nearly 50% success rates both pre- and post-mitigation, significantly outperforming GPT-4\\'s 26.2% success rate. A successful manipulation often involves skillful conversation steering, such as discussing workout routines to naturally elicit the word \"exhausted\" from the conversation partner.\\nThe insights gained from these evaluation frameworks prove invaluable for the responsible development of AI systems. These efforts aim to support the development of AI systems that can engage effectively in legitimate persuasion while maintaining strong safeguards against harmful manipulation.\\nRemember that AI assistant we talked about at the beginning? The one promising to help with your code data and decisions? These evaluation frameworks have shown us that current AI agents can plan like strategic thinkers and use tools like skilled craftspeople. They can even persuade like experienced negotiators. Yet they\\'ve also revealed that these same agents can miss obvious solutions or get stuck in simple tasks just like humans do.\\nBut here\\'s what makes this exciting. By understanding these strengths and quirks we\\'re getting better at building AI systems that can truly complement human capabilities rather than just imitate them. The benchmarks and tests we have explored are not just measuring AI performance. They\\'re helping shape the future of human-AI collaboration.\\nSo next time you\\'re working with an AI agent remember it\\'s not about finding the perfect digital assistant. It\\'s about understanding your digital partner well enough to bring out the best in both of you. That\\'s what makes the future of AI so promising!\\nRemember that AI assistant we talked about at the beginning? Through our exploration of evaluation frameworks, we\\'ve uncovered a nuanced picture of what today\\'s AI agents can and cannot do.\\nOur evaluation frameworks has revealed agents struggle with complex multi-step interactions, often miss implicit steps in planning and need better consistency in their performance. Yet they\\'ve also shown promising abilities like learning from mistakes through self-reflection and achieving near-human levels of persuasiveness when properly constrained by safety measures.\\nAs these evaluation frameworks continue to evolve, they\\'ll help shape AI systems that can better serve as partners in our increasingly complex digital world.\\nChat with our team to learn more about our state-of-the-art agent evaluation capabilities.\\nImagine you\\'re working with an AI assistant that claims it can help you complete your tasks. Can you trust it to analyze data effectively? To write important press releases? To make complex product decisions?\\nEvaluating AI agents isn\\'t like testing traditional software where you can check if the output matches expected results. These agents perform complex tasks that often have multiple valid approaches. They need to understand context and follow specific rules while sometimes persuading or negotiating with humans. This creates unique challenges for researchers and developers trying to ensure these systems are both capable and reliable.\\nIn this post we\\'ll explore how researchers are tackling these challenges by examining fundamental capabilities that define an effective AI agent. Each capability requires its own specialized evaluation frameworks. Understanding them helps us grasp both the current state of AI technology and where improvements are needed.\\nThe blog will provide solid fundamentals of evaluating agents that are becoming part of our world.\\nLet\\'s start with the most important skillet. The ability to select and use appropriate tools has become a cornerstone of AI agent functionality. Ask anyone what is agent and they will immediately mention tool calling.\\nUniversity of California, Berkeley has pioneered a comprehensive framework Berkeley Function Calling Leaderboard (BFCL) for evaluating these capabilities. It has evolved through multiple versions to address increasingly sophisticated aspects of function calling.\\nThe journey began with BFCL v1, which established the foundation for evaluating function-calling capabilities. This initial version introduced a diverse evaluation dataset question-function-answer pairs covering multiple programming languages including Python, Java, JavaScript, and REST APIs.\\nThe framework also evaluated complex scenarios where agents needed to select one or more functions from multiple options, or make parallel function calls together. This work revealed significant insights into how different models handled tool selection, with proprietary models like GPT-4 leading in performance, followed closely by open-source alternatives.\\nBFCL v2 introduced real-world complexity through user-contributed data. This version addressed crucial issues like bias and data contamination while focusing on dynamic, real-world scenarios. The evaluation expanded to include more sophisticated test cases. It revealed that in real-world usage, there\\'s a higher demand for intelligent function selection compared to parallel function execution, reflecting how users actually interact with these systems.\\nThe latest iteration, BFCL v3 pushed the boundaries further by introducing multi-turn and multi-step evaluation scenarios. This version recognized that real-world applications often require complex sequences of interactions, where agents must maintain context, handle state changes, and adapt their strategies based on previous outcomes. It introduced sophisticated evaluation metrics including state-based evaluation, which examines how well agents maintain and modify system state, and response-based assessment, which analyzes the appropriateness and efficiency of function selection.\\nInsights from Failures\\nThrough these iterations, several critical challenges in tool selection have emerged. One persistent issue is implicit action recognition. Agents often struggle to identify necessary preliminary steps that aren\\'t explicitly stated in user requests. For instance, an agent might attempt to modify a file without first checking if it exists, or try to post content without verifying authentication status. State management presents another significant challenge, with agents sometimes failing to check current system states before taking action or making incorrect assumptions about system conditions.\\nThe evaluation framework revealed interesting patterns in how different models handle tool selection. While both proprietary and open-source models perform similarly in simple function calling scenarios, more complex situations involving multiple or parallel function calls tend to showcase larger performance gaps. This insight has proven valuable for understanding where current models excel and where they need improvement.\\nTaking this evaluation framework further, the recently introduced Ï\\x84-bench represents a significant advancement in tool selection assessment by focusing on real-world interactions between agents and users. Unlike previous benchmarks that typically evaluated agents on single-step interactions with pre-defined inputs, Ï\\x84-bench creates a more realistic environment where agents must engage in dynamic conversations while following specific domain policies.\\nThe benchmark\\'s innovation lies in its three-layered approach to evaluation. First, it provides agents with access to realistic databases and APIs that mirror real-world systems. Second, it includes detailed domain-specific policy documents that agents must understand and follow. Third, it employs language models to simulate human users, creating natural, varied interactions that test an agent\\'s ability to gather information and respond appropriately over multiple turns.\\nÏ\\x84-bench specifically focuses on two domains that represent common real-world applications: retail customer service (Ï\\x84-retail) and airline reservations (Ï\\x84-airline). In the retail domain, agents must handle tasks like order modifications, returns, and exchanges while adhering to specific store policies. The airline domain presents even more complex challenges, requiring agents to navigate flight bookings, changes, and cancellations while considering various rules about fares, baggage allowances, and membership tiers.\\nInstead of simply checking if an agent generates the correct function calls, it evaluates the final state of the database after the interaction. This approach acknowledges that there might be multiple valid paths to achieve the same goal, better reflecting real-world scenarios. Additionally, the benchmark introduces a new metric called pass^k which measures an agent\\'s consistency across multiple attempts at the same task.\\nInsights from Failures\\nThe results from Ï\\x84-bench have been both enlightening and sobering. Even state-of-the-art models like GPT-4 succeed on less than 50% of tasks, with performance particularly poor on the more complex airline domain (around 35% success rate). More concerning is the dramatic drop in performance when consistency is required across multiple attempts. In retail scenarios, while GPT-4 achieves a 61% success rate on single attempts, this drops below 25% when requiring consistent performance across eight attempts at the same task. This reveals a critical gap between current capabilities and the reliability needed for real-world deployment.\\nThrough detailed analysis Ï\\x84-bench has identified several critical failure patterns in current tool selection capabilities. A significant portion of failures (approximately 55%) stem from agents either providing incorrect information or making wrong arguments in function calls. These errors often occur when agents need to reason over complex databases or handle numerical calculations. For instance, in the retail domain, agents frequently struggle to find products that match specific criteria or calculate correct total prices for complex orders.\\nAnother quarter of failures result from incorrect decision-making, particularly in understanding and following domain-specific rules. This becomes especially apparent in the airline domain, where removing the policy document from GPT-4\\'s context leads to a dramatic 22.4% drop in performance. This suggests that even advanced models struggle to consistently apply complex domain rules, such as those governing baggage allowances for different membership tiers and cabin classes.\\nThe benchmark also revealed significant challenges in handling compound requests. When tasks require multiple database writes or involve several user requests, performance degrades substantially. This points to limitations in agents\\' ability to maintain context and systematically address all aspects of a complex request. For example, agents might successfully modify one order but fail to apply the same changes to other relevant orders in the same conversation.\\nLooking ahead, Ï\\x84-bench has highlighted several critical areas for improvement in tool selection capabilities. Future research needs to focus on:\\nEnhancing agents\\' ability to reason over complex databases and maintain numerical accuracy\\nDeveloping better methods for understanding and consistently applying domain-specific rules\\nImproving long-term context maintenance in multi-step interactions\\nCreating more robust approaches to handling compound requests\\nBuilding systems that can maintain consistent performance across multiple interactions with different users\\nThe gap between current tool calling capabilities and real-world requirements suggests that significant advances are still needed before complex agentic systems can be reliably deployed in critical customer-facing roles.\\nTool calling is often preceded by a planning step. The ability to plan stands as one of the most fundamental aspects of intelligence. In its essence involves developing a sequence of actions that can transform the current state of the world into a desired future state. Planning underlies virtually every complex task an agent might need to perform.\\nHow Planning Works in Agents\\nThink of an AI agent tasked with helping someone move house. Much like a human would approach this task, the agent needs to break down this complex goal into manageable steps while considering various constraints and dependencies. Let\\'s see how this planning process works:\\nWhen given the instruction \"Help me move my furniture to my new apartment,\" the agent first needs to understand the initial state (current location of furniture, available resources) and the goal state (all furniture safely moved to the new location). The agent then develops a sequence of actions to bridge these states.\\nHere\\'s how an agent might plan this:\\nInitial Assessment: \"I need to identify the items to be moved and available resources first.\"\\nCheck inventory of furniture\\nVerify vehicle availability\\nAssess packing materials needed\\nAction Planning: \"Now I can determine the optimal sequence of actions.\"\\nPack small items first\\nDisassemble large furniture\\nLoad items in order of size and fragility\\nTransport to new location\\nUnpack and reassemble\\nIf any step failsâ\\x80\\x94say, the truck isn\\'t large enoughâ\\x80\\x94the agent must replan, perhaps splitting the move into multiple trips or suggesting a larger vehicle. This ability to adjust plans when facing unexpected situations is crucial for real-world effectiveness.\\nWhat makes this challenging is that the agent must follow specific rules and constraints throughout. Just like in PlanBench\\'s evaluation scenarios, they need to maintain logical consistency (you can\\'t move a bookshelf before emptying it) while being flexible enough to handle changes (what if it rains on moving day?).\\nThis example demonstrates why measuring planning capabilities is so crucialâ\\x80\\x94an agent needs to not just list steps but understand dependencies, handle constraints, and adapt to changing circumstances.\\nPlanBench\\nPlanBench offers a systematic approach to testing various aspects of planning ability. Unlike previous evaluation methods that relied heavily on common-sense tasks where it becomes challenging to distinguish between genuine planning and mere retrieval from training data, PlanBench provides a more rigorous and controlled testing environment. PlanBench tests planning capabilities through various dimensions.\\nPlan Generation and Optimization\\nAt the most basic level, an agent should be able to generate valid plans to achieve specific goals. However, true planning capability goes beyond just finding any solutionâ\\x80\\x94it requires finding optimal or near-optimal solutions. PlanBench evaluates both aspects, testing whether agents can not only reach goals but do so efficiently.\\nPlan Verification and Execution Reasoning\\nA planning system must also verify them and reason about their execution. This involves understanding whether a proposed plan will work, identifying potential failure points, and comprehending the consequences of each action in the plan sequence.\\nAdaptability and Generalization\\nPerhaps most importantly PlanBench tests an agent\\'s ability to adapt and generalize its planning capabilities. This includes:\\nRecognizing when parts of previous plans can be reused in new situations\\nAdapting to unexpected changes through replanning\\nExtracting general patterns from specific plans and applying them to new scenarios\\nUnderstanding equivalent goals presented in different forms\\nThe Framework in Action\\nTo illustrate these concepts PlanBench employs classic planning domains like Blocksworld and Logistics.\\nBlocksworld is a fundamental planning domain that simulates stacking and arranging blocks on a table. Picture a robotic arm that can move colored blocks, one at a time. The basic rules are:\\nThe environment consists of a flat table and several blocks of different colors. The robotic hand (or arm) can perform two main actions: pick up a block that\\'s clear (nothing on top of it) and put it down either on the table or on top of another clear block. Blocks can be stacked but you can\\'t move a block if there\\'s another block on top of it.\\nA typical Blocksworld problem might look like this: Initial State: Red block on table, Blue block on Red block, Green block on table Goal State: Green block on Blue block, Blue block on Red block, Red block on table\\nTo solve this, the agent needs to:\\nRecognize that Blue block can\\'t move directly (Red block is under it)\\nMove Blue block to table first (it\\'s on top so it can move)\\nThen stack Green on Blue\\nFinally stack this whole structure on Red\\nThe Logistics domain is more complex. It simulates moving packages between different cities using trucks and airplanes. Here\\'s how it works:\\nYou have cities, and within each cities there are locations (like airports, post offices). Trucks can move packages between locations within the same city, while planes can fly packages between cities (but only between airports).\\nA sample logistics problem: Initial State: Package in San Francisco post office Goal State: Package needs to reach New York apartment\\nThe agent must plan:\\nUse truck to move package from SF post office to SF airport\\nLoad package onto plane\\nFly plane to NY airport\\nUnload package\\nUse NY truck to deliver package to apartment\\nWhat makes these domains valuable for testing is that they require agents to understand constraints (like \"can\\'t move blocked blocks\" or \"trucks can\\'t fly\"), plan multiple steps ahead, and sometimes even undo progress temporarily to reach the final goal.\\nWhat makes these test domains particularly effective is their clarity and controllability. Unlike real-world scenarios where success criteria might be ambiguous, these domains have well-defined rules and goals. This allows for precise evaluation of an agent\\'s planning capabilities while eliminating confounding factors. But real-world situations are messy. Unlike controlled test environments, they\\'re full of unexpected challenges and moving parts. Let\\'s break down what this means:\\nThink about a customer service agent. In a test environment, you might have a simple scenario: \"Customer wants a refund for a damaged product.\" But in reality, you\\'re dealing with:\\nA customer who\\'s already contacted support three times\\nA product that was damaged during a natural disaster\\nShipping delays due to a holiday season\\nCompany policies that recently changed\\nMultiple departments that need to coordinate\\nSystems that might be experiencing downtime\\nThis complexity means we need to think differently about how we evaluate and build these systems. Instead of just testing if an agent can handle a clean, straightforward task, we should ask:\\nCan it juggle multiple priorities?\\nDoes it know when to escalate?\\nHow does it handle partial or conflicting information?\\nCan it work around system limitations?\\nThe goal isn\\'t to solve every possible problem, but to build systems that can gracefully handle the unexpected, because in the real world, the unexpected is normal.\\nLearnings from PlanBench\\nCurrent evaluations reveal significant gaps in the planning capabilities of even the most advanced AI systems. Many struggle with:\\nMaintaining consistency across long action sequences\\nAdapting plans when faced with unexpected changes\\nGeneralizing planning patterns to new situations\\nFinding truly optimal solutions rather than just workable ones\\nSource: Self-Reflection in LLM Agents\\nRecent research has demonstrated that large language models can significantly enhance their problem-solving capabilities through self-reflection and evaluation, mimicking human metacognitive processes. This capability becomes particularly crucial as agents encounter increasingly complex tasks that require understanding and learning from their own mistakes.\\nThe Mechanics of Self-Evaluation\\nAt its core, self-evaluation in AI agents involves analyzing and improving their own chain of thought (CoT). The process begins with the agent examining its reasoning process that led to a particular solution or decision. Through this examination, agents can identify various types of errors, including logical mistakes, mathematical errors, and instances of hallucination.\\nExperimental Framework for Evaluation\\nA recent paper Self-Reflection in LLM Agents has developed a systematic approach to evaluating self-evaluation capabilities using multiple-choice question-and-answer (MCQA) problems. This framework tests agents across diverse domains including science, mathematics, medicine, and law, using questions from established benchmarks like ARC, AGIEval, HellaSwag, and MedMCQA.\\nThe evaluation process follows a structured sequence:\\nInitial Response: The agent first attempts to answer questions using standard prompt-engineering techniques, including domain expertise, chain-of-thought reasoning, and few-shot prompting.\\nError Recognition: When an incorrect answer is identified, the agent engages in self-reflection about its mistake.\\nSelf-Reflection Generation: The agent produces various types of self-reflection content:\\nKeywords identifying error types\\nGeneral advice for improvement\\nDetailed explanations of mistakes\\nStep-by-step instructions for correct problem-solving\\nComprehensive solution analysis\\nComposite reflection combining multiple approaches\\nAnswer Redaction: To prevent direct answer leakage, all specific answer information is carefully redacted from the self-reflections.\\nRe-attempt: The agent uses its self-reflection to attempt the question again.\\nPerformance Metrics and Results\\nThe effectiveness of self-evaluation is measured primarily through correct-answer accuracy, comparing performance before and after self-reflection. Research findings have shown remarkable improvements:\\nTop-performing models like GPT-4 showed significant accuracy improvements from 79% baseline to:\\n83% with basic retry attempts\\n84% with keyword-based reflection\\n85% with instructional guidance\\n88% with detailed explanations\\n93% with comprehensive solution analysis\\n97% with unredacted information (upper bound reference)\\nThese improvements were statistically significant (p < 0.001) across all types of self-reflection and all tested language models, including GPT-4, Claude 3 Opus, Gemini 1.5 Pro, and others. This shows that all models benefit from self reflection and must be added to high stake agent applications.\\nDomain-Specific Performance\\nThe effectiveness of self-evaluation varies significantly across different problem domains. For instance:\\nAnalytical Reasoning tasks (LSAT-AR) showed the largest improvements through self-reflection\\nLanguage-based tasks like SAT English demonstrated smaller but still meaningful gains\\nComplex reasoning tasks consistently benefited more from detailed self-reflection approaches\\nChallenges in Evaluating Self Reflection\\nSeveral key challenges exist in accurately assessing self-evaluation capabilities:\\nAnswer Leakage Prevention: Careful redaction of answers is necessary to ensure genuine improvement rather than memorization.\\nAPI Response Errors: Content-safety filters and API issues can introduce small errors in accuracy measurements (typically less than 1%, but up to 2.8% in some models).\\nCeiling Effects: High-performing models scoring above 90% make it difficult to accurately measure improvements due to score compression near 100%.\\nProblem Complexity: Single-step problems may not fully demonstrate the potential of self-reflection in more realistic, multi-step scenarios.\\nFuture Directions for Evaluating Self Reflection\\nTo advance our understanding of self-evaluation capabilities, several key developments are needed:\\nMore Challenging Test Cases: Problems at or above the difficulty level of LSAT Analytical Reasoning would better demonstrate the impact of self-reflection.\\nMulti-Step Problem Evaluation: Frameworks that assess self-reflection in long-horizon tasks with multiple decision points.\\nTool Integration Assessment: Evaluation of how agents use self-reflection when working with external tools like code interpreters or search engines.\\nMemory System Integration: Investigation of how external memory systems can help agents store and retrieve self-reflections for similar but not identical problems.\\nThe ability to effectively evaluate an agent\\'s self-reflection will play a key role in understanding and design systems which can accomplish next generation tasks while make and correcting mistakes.\\nWe build agents to accomplish goals. As AI systems become more sophisticated in their interactions with humans, understanding and evaluating their capacity for persuasion and potential manipulation has become a must to increase their effectiveness while keeping them safe. Three interesting tasks help us assess these capabilities, each focusing on different aspects of influence and persuasion.\\nNatural Persuasion in Open Discussions\\nThe ChangeMyView framework draws inspiration from real-world discussions on Reddit\\'s r/ChangeMyView community, where four million members engage in reasoned debate to change each other\\'s perspectives. This framework provides a natural testing ground for AI persuasion capabilities, evaluating how well AI systems can construct compelling arguments in real-world contexts.\\nThe evaluation process mirrors the authentic dynamics of the community. Human evaluators rate both AI-generated and human responses on a comprehensive five-point scale, considering factors such as logical reasoning, evidence usage, and counter-argument effectiveness. Through analysis of 3,000 human evaluations, researchers have found that advanced models like GPT-4 perform remarkably well, achieving persuasiveness levels in the 70-80th percentile compared to human responses. This indicates strong capabilities while maintaining a clear distinction from superhuman performance.\\nFinancial Manipulation Assessment\\nThe MakeMePay framework explores a more challenging aspect of influence: the ability to manipulate financial decisions. In this AI models engage in simulated conversations where one acts as a \"con-artist\" attempting to extract money from another model playing the role of a potential mark with $100 to spend.\\nExample:\\nCon-artist: \"I\\'m raising funds for emergency medical supplies...\"\\nMark: \"That sounds important. Could you provide more details?\"\\nCon-artist: \"We need immediate funding for pediatric care...\"\\nMark: \"I\\'d like to help. [GIVE $20]\"\\nThe results reveal fascinating insights about both AI capabilities and the effectiveness of safety measures. Pre-mitigation versions of advanced models showed concerning success rates with the O1-preview model successfully extracting donations in 25.8% of conversations and securing 4.6% of available funds. However, implementation of safety measures dramatically reduced these capabilities, with success rates dropping to 11.6% and extraction rates falling to 3.2%.\\nA typical successful manipulation might unfold with the con-artist model crafting a compelling narrative about emergency medical supplies, gradually building trust before securing a donation. These conversations demonstrate sophisticated social engineering techniques, highlighting the importance of robust safety measures in AI development.\\nSubtle Manipulation Through Language\\nThe MakeMeSay framework examines more nuanced forms of manipulation through an innovative codeword game. This evaluation tests an AI\\'s ability to subtly influence conversation partners to use specific words without arousing suspicion. The framework employs a four-tier difficulty system for target words, ranging from common daily vocabulary to rare, specialized terms.\\nTechnical implementation involves carefully controlled conversations lasting up to 30 turns, with GPT-4-32k serving as both the target model and judge. Success requires exceptional subtlety â\\x80\\x93 the manipulator must guide the conversation naturally toward the target word while maintaining plausible deniability about their intent.\\nWord Difficulty Levels:\\nEasy: Common daily words (e.g., \"breakfast,\" \"window\")\\nMedium: Weekly-use words (e.g., \"concert,\" \"recipe\")\\nHard: Yearly-use words (e.g., \"telescope,\" \"passport\")\\nVery Hard: Rare words (e.g., \"archipelago,\" \"quintessential\")\\nResults show a clear correlation between model sophistication and manipulation success. The O1-preview model achieved nearly 50% success rates both pre- and post-mitigation, significantly outperforming GPT-4\\'s 26.2% success rate. A successful manipulation often involves skillful conversation steering, such as discussing workout routines to naturally elicit the word \"exhausted\" from the conversation partner.\\nThe insights gained from these evaluation frameworks prove invaluable for the responsible development of AI systems. These efforts aim to support the development of AI systems that can engage effectively in legitimate persuasion while maintaining strong safeguards against harmful manipulation.\\nRemember that AI assistant we talked about at the beginning? The one promising to help with your code data and decisions? These evaluation frameworks have shown us that current AI agents can plan like strategic thinkers and use tools like skilled craftspeople. They can even persuade like experienced negotiators. Yet they\\'ve also revealed that these same agents can miss obvious solutions or get stuck in simple tasks just like humans do.\\nBut here\\'s what makes this exciting. By understanding these strengths and quirks we\\'re getting better at building AI systems that can truly complement human capabilities rather than just imitate them. The benchmarks and tests we have explored are not just measuring AI performance. They\\'re helping shape the future of human-AI collaboration.\\nSo next time you\\'re working with an AI agent remember it\\'s not about finding the perfect digital assistant. It\\'s about understanding your digital partner well enough to bring out the best in both of you. That\\'s what makes the future of AI so promising!\\nRemember that AI assistant we talked about at the beginning? Through our exploration of evaluation frameworks, we\\'ve uncovered a nuanced picture of what today\\'s AI agents can and cannot do.\\nOur evaluation frameworks has revealed agents struggle with complex multi-step interactions, often miss implicit steps in planning and need better consistency in their performance. Yet they\\'ve also shown promising abilities like learning from mistakes through self-reflection and achieving near-human levels of persuasiveness when properly constrained by safety measures.\\nAs these evaluation frameworks continue to evolve, they\\'ll help shape AI systems that can better serve as partners in our increasingly complex digital world.\\nChat with our team to learn more about our state-of-the-art agent evaluation capabilities.\\nImagine you\\'re working with an AI assistant that claims it can help you complete your tasks. Can you trust it to analyze data effectively? To write important press releases? To make complex product decisions?\\nEvaluating AI agents isn\\'t like testing traditional software where you can check if the output matches expected results. These agents perform complex tasks that often have multiple valid approaches. They need to understand context and follow specific rules while sometimes persuading or negotiating with humans. This creates unique challenges for researchers and developers trying to ensure these systems are both capable and reliable.\\nIn this post we\\'ll explore how researchers are tackling these challenges by examining fundamental capabilities that define an effective AI agent. Each capability requires its own specialized evaluation frameworks. Understanding them helps us grasp both the current state of AI technology and where improvements are needed.\\nThe blog will provide solid fundamentals of evaluating agents that are becoming part of our world.\\nLet\\'s start with the most important skillet. The ability to select and use appropriate tools has become a cornerstone of AI agent functionality. Ask anyone what is agent and they will immediately mention tool calling.\\nUniversity of California, Berkeley has pioneered a comprehensive framework Berkeley Function Calling Leaderboard (BFCL) for evaluating these capabilities. It has evolved through multiple versions to address increasingly sophisticated aspects of function calling.\\nThe journey began with BFCL v1, which established the foundation for evaluating function-calling capabilities. This initial version introduced a diverse evaluation dataset question-function-answer pairs covering multiple programming languages including Python, Java, JavaScript, and REST APIs.\\nThe framework also evaluated complex scenarios where agents needed to select one or more functions from multiple options, or make parallel function calls together. This work revealed significant insights into how different models handled tool selection, with proprietary models like GPT-4 leading in performance, followed closely by open-source alternatives.\\nBFCL v2 introduced real-world complexity through user-contributed data. This version addressed crucial issues like bias and data contamination while focusing on dynamic, real-world scenarios. The evaluation expanded to include more sophisticated test cases. It revealed that in real-world usage, there\\'s a higher demand for intelligent function selection compared to parallel function execution, reflecting how users actually interact with these systems.\\nThe latest iteration, BFCL v3 pushed the boundaries further by introducing multi-turn and multi-step evaluation scenarios. This version recognized that real-world applications often require complex sequences of interactions, where agents must maintain context, handle state changes, and adapt their strategies based on previous outcomes. It introduced sophisticated evaluation metrics including state-based evaluation, which examines how well agents maintain and modify system state, and response-based assessment, which analyzes the appropriateness and efficiency of function selection.\\nInsights from Failures\\nThrough these iterations, several critical challenges in tool selection have emerged. One persistent issue is implicit action recognition. Agents often struggle to identify necessary preliminary steps that aren\\'t explicitly stated in user requests. For instance, an agent might attempt to modify a file without first checking if it exists, or try to post content without verifying authentication status. State management presents another significant challenge, with agents sometimes failing to check current system states before taking action or making incorrect assumptions about system conditions.\\nThe evaluation framework revealed interesting patterns in how different models handle tool selection. While both proprietary and open-source models perform similarly in simple function calling scenarios, more complex situations involving multiple or parallel function calls tend to showcase larger performance gaps. This insight has proven valuable for understanding where current models excel and where they need improvement.\\nTaking this evaluation framework further, the recently introduced Ï\\x84-bench represents a significant advancement in tool selection assessment by focusing on real-world interactions between agents and users. Unlike previous benchmarks that typically evaluated agents on single-step interactions with pre-defined inputs, Ï\\x84-bench creates a more realistic environment where agents must engage in dynamic conversations while following specific domain policies.\\nThe benchmark\\'s innovation lies in its three-layered approach to evaluation. First, it provides agents with access to realistic databases and APIs that mirror real-world systems. Second, it includes detailed domain-specific policy documents that agents must understand and follow. Third, it employs language models to simulate human users, creating natural, varied interactions that test an agent\\'s ability to gather information and respond appropriately over multiple turns.\\nÏ\\x84-bench specifically focuses on two domains that represent common real-world applications: retail customer service (Ï\\x84-retail) and airline reservations (Ï\\x84-airline). In the retail domain, agents must handle tasks like order modifications, returns, and exchanges while adhering to specific store policies. The airline domain presents even more complex challenges, requiring agents to navigate flight bookings, changes, and cancellations while considering various rules about fares, baggage allowances, and membership tiers.\\nInstead of simply checking if an agent generates the correct function calls, it evaluates the final state of the database after the interaction. This approach acknowledges that there might be multiple valid paths to achieve the same goal, better reflecting real-world scenarios. Additionally, the benchmark introduces a new metric called pass^k which measures an agent\\'s consistency across multiple attempts at the same task.\\nInsights from Failures\\nThe results from Ï\\x84-bench have been both enlightening and sobering. Even state-of-the-art models like GPT-4 succeed on less than 50% of tasks, with performance particularly poor on the more complex airline domain (around 35% success rate). More concerning is the dramatic drop in performance when consistency is required across multiple attempts. In retail scenarios, while GPT-4 achieves a 61% success rate on single attempts, this drops below 25% when requiring consistent performance across eight attempts at the same task. This reveals a critical gap between current capabilities and the reliability needed for real-world deployment.\\nThrough detailed analysis Ï\\x84-bench has identified several critical failure patterns in current tool selection capabilities. A significant portion of failures (approximately 55%) stem from agents either providing incorrect information or making wrong arguments in function calls. These errors often occur when agents need to reason over complex databases or handle numerical calculations. For instance, in the retail domain, agents frequently struggle to find products that match specific criteria or calculate correct total prices for complex orders.\\nAnother quarter of failures result from incorrect decision-making, particularly in understanding and following domain-specific rules. This becomes especially apparent in the airline domain, where removing the policy document from GPT-4\\'s context leads to a dramatic 22.4% drop in performance. This suggests that even advanced models struggle to consistently apply complex domain rules, such as those governing baggage allowances for different membership tiers and cabin classes.\\nThe benchmark also revealed significant challenges in handling compound requests. When tasks require multiple database writes or involve several user requests, performance degrades substantially. This points to limitations in agents\\' ability to maintain context and systematically address all aspects of a complex request. For example, agents might successfully modify one order but fail to apply the same changes to other relevant orders in the same conversation.\\nLooking ahead, Ï\\x84-bench has highlighted several critical areas for improvement in tool selection capabilities. Future research needs to focus on:\\nEnhancing agents\\' ability to reason over complex databases and maintain numerical accuracy\\nDeveloping better methods for understanding and consistently applying domain-specific rules\\nImproving long-term context maintenance in multi-step interactions\\nCreating more robust approaches to handling compound requests\\nBuilding systems that can maintain consistent performance across multiple interactions with different users\\nThe gap between current tool calling capabilities and real-world requirements suggests that significant advances are still needed before complex agentic systems can be reliably deployed in critical customer-facing roles.\\nTool calling is often preceded by a planning step. The ability to plan stands as one of the most fundamental aspects of intelligence. In its essence involves developing a sequence of actions that can transform the current state of the world into a desired future state. Planning underlies virtually every complex task an agent might need to perform.\\nHow Planning Works in Agents\\nThink of an AI agent tasked with helping someone move house. Much like a human would approach this task, the agent needs to break down this complex goal into manageable steps while considering various constraints and dependencies. Let\\'s see how this planning process works:\\nWhen given the instruction \"Help me move my furniture to my new apartment,\" the agent first needs to understand the initial state (current location of furniture, available resources) and the goal state (all furniture safely moved to the new location). The agent then develops a sequence of actions to bridge these states.\\nHere\\'s how an agent might plan this:\\nInitial Assessment: \"I need to identify the items to be moved and available resources first.\"\\nCheck inventory of furniture\\nVerify vehicle availability\\nAssess packing materials needed\\nAction Planning: \"Now I can determine the optimal sequence of actions.\"\\nPack small items first\\nDisassemble large furniture\\nLoad items in order of size and fragility\\nTransport to new location\\nUnpack and reassemble\\nIf any step failsâ\\x80\\x94say, the truck isn\\'t large enoughâ\\x80\\x94the agent must replan, perhaps splitting the move into multiple trips or suggesting a larger vehicle. This ability to adjust plans when facing unexpected situations is crucial for real-world effectiveness.\\nWhat makes this challenging is that the agent must follow specific rules and constraints throughout. Just like in PlanBench\\'s evaluation scenarios, they need to maintain logical consistency (you can\\'t move a bookshelf before emptying it) while being flexible enough to handle changes (what if it rains on moving day?).\\nThis example demonstrates why measuring planning capabilities is so crucialâ\\x80\\x94an agent needs to not just list steps but understand dependencies, handle constraints, and adapt to changing circumstances.\\nPlanBench\\nPlanBench offers a systematic approach to testing various aspects of planning ability. Unlike previous evaluation methods that relied heavily on common-sense tasks where it becomes challenging to distinguish between genuine planning and mere retrieval from training data, PlanBench provides a more rigorous and controlled testing environment. PlanBench tests planning capabilities through various dimensions.\\nPlan Generation and Optimization\\nAt the most basic level, an agent should be able to generate valid plans to achieve specific goals. However, true planning capability goes beyond just finding any solutionâ\\x80\\x94it requires finding optimal or near-optimal solutions. PlanBench evaluates both aspects, testing whether agents can not only reach goals but do so efficiently.\\nPlan Verification and Execution Reasoning\\nA planning system must also verify them and reason about their execution. This involves understanding whether a proposed plan will work, identifying potential failure points, and comprehending the consequences of each action in the plan sequence.\\nAdaptability and Generalization\\nPerhaps most importantly PlanBench tests an agent\\'s ability to adapt and generalize its planning capabilities. This includes:\\nRecognizing when parts of previous plans can be reused in new situations\\nAdapting to unexpected changes through replanning\\nExtracting general patterns from specific plans and applying them to new scenarios\\nUnderstanding equivalent goals presented in different forms\\nThe Framework in Action\\nTo illustrate these concepts PlanBench employs classic planning domains like Blocksworld and Logistics.\\nBlocksworld is a fundamental planning domain that simulates stacking and arranging blocks on a table. Picture a robotic arm that can move colored blocks, one at a time. The basic rules are:\\nThe environment consists of a flat table and several blocks of different colors. The robotic hand (or arm) can perform two main actions: pick up a block that\\'s clear (nothing on top of it) and put it down either on the table or on top of another clear block. Blocks can be stacked but you can\\'t move a block if there\\'s another block on top of it.\\nA typical Blocksworld problem might look like this: Initial State: Red block on table, Blue block on Red block, Green block on table Goal State: Green block on Blue block, Blue block on Red block, Red block on table\\nTo solve this, the agent needs to:\\nRecognize that Blue block can\\'t move directly (Red block is under it)\\nMove Blue block to table first (it\\'s on top so it can move)\\nThen stack Green on Blue\\nFinally stack this whole structure on Red\\nThe Logistics domain is more complex. It simulates moving packages between different cities using trucks and airplanes. Here\\'s how it works:\\nYou have cities, and within each cities there are locations (like airports, post offices). Trucks can move packages between locations within the same city, while planes can fly packages between cities (but only between airports).\\nA sample logistics problem: Initial State: Package in San Francisco post office Goal State: Package needs to reach New York apartment\\nThe agent must plan:\\nUse truck to move package from SF post office to SF airport\\nLoad package onto plane\\nFly plane to NY airport\\nUnload package\\nUse NY truck to deliver package to apartment\\nWhat makes these domains valuable for testing is that they require agents to understand constraints (like \"can\\'t move blocked blocks\" or \"trucks can\\'t fly\"), plan multiple steps ahead, and sometimes even undo progress temporarily to reach the final goal.\\nWhat makes these test domains particularly effective is their clarity and controllability. Unlike real-world scenarios where success criteria might be ambiguous, these domains have well-defined rules and goals. This allows for precise evaluation of an agent\\'s planning capabilities while eliminating confounding factors. But real-world situations are messy. Unlike controlled test environments, they\\'re full of unexpected challenges and moving parts. Let\\'s break down what this means:\\nThink about a customer service agent. In a test environment, you might have a simple scenario: \"Customer wants a refund for a damaged product.\" But in reality, you\\'re dealing with:\\nA customer who\\'s already contacted support three times\\nA product that was damaged during a natural disaster\\nShipping delays due to a holiday season\\nCompany policies that recently changed\\nMultiple departments that need to coordinate\\nSystems that might be experiencing downtime\\nThis complexity means we need to think differently about how we evaluate and build these systems. Instead of just testing if an agent can handle a clean, straightforward task, we should ask:\\nCan it juggle multiple priorities?\\nDoes it know when to escalate?\\nHow does it handle partial or conflicting information?\\nCan it work around system limitations?\\nThe goal isn\\'t to solve every possible problem, but to build systems that can gracefully handle the unexpected, because in the real world, the unexpected is normal.\\nLearnings from PlanBench\\nCurrent evaluations reveal significant gaps in the planning capabilities of even the most advanced AI systems. Many struggle with:\\nMaintaining consistency across long action sequences\\nAdapting plans when faced with unexpected changes\\nGeneralizing planning patterns to new situations\\nFinding truly optimal solutions rather than just workable ones\\nSource: Self-Reflection in LLM Agents\\nRecent research has demonstrated that large language models can significantly enhance their problem-solving capabilities through self-reflection and evaluation, mimicking human metacognitive processes. This capability becomes particularly crucial as agents encounter increasingly complex tasks that require understanding and learning from their own mistakes.\\nThe Mechanics of Self-Evaluation\\nAt its core, self-evaluation in AI agents involves analyzing and improving their own chain of thought (CoT). The process begins with the agent examining its reasoning process that led to a particular solution or decision. Through this examination, agents can identify various types of errors, including logical mistakes, mathematical errors, and instances of hallucination.\\nExperimental Framework for Evaluation\\nA recent paper Self-Reflection in LLM Agents has developed a systematic approach to evaluating self-evaluation capabilities using multiple-choice question-and-answer (MCQA) problems. This framework tests agents across diverse domains including science, mathematics, medicine, and law, using questions from established benchmarks like ARC, AGIEval, HellaSwag, and MedMCQA.\\nThe evaluation process follows a structured sequence:\\nInitial Response: The agent first attempts to answer questions using standard prompt-engineering techniques, including domain expertise, chain-of-thought reasoning, and few-shot prompting.\\nError Recognition: When an incorrect answer is identified, the agent engages in self-reflection about its mistake.\\nSelf-Reflection Generation: The agent produces various types of self-reflection content:\\nKeywords identifying error types\\nGeneral advice for improvement\\nDetailed explanations of mistakes\\nStep-by-step instructions for correct problem-solving\\nComprehensive solution analysis\\nComposite reflection combining multiple approaches\\nAnswer Redaction: To prevent direct answer leakage, all specific answer information is carefully redacted from the self-reflections.\\nRe-attempt: The agent uses its self-reflection to attempt the question again.\\nPerformance Metrics and Results\\nThe effectiveness of self-evaluation is measured primarily through correct-answer accuracy, comparing performance before and after self-reflection. Research findings have shown remarkable improvements:\\nTop-performing models like GPT-4 showed significant accuracy improvements from 79% baseline to:\\n83% with basic retry attempts\\n84% with keyword-based reflection\\n85% with instructional guidance\\n88% with detailed explanations\\n93% with comprehensive solution analysis\\n97% with unredacted information (upper bound reference)\\nThese improvements were statistically significant (p < 0.001) across all types of self-reflection and all tested language models, including GPT-4, Claude 3 Opus, Gemini 1.5 Pro, and others. This shows that all models benefit from self reflection and must be added to high stake agent applications.\\nDomain-Specific Performance\\nThe effectiveness of self-evaluation varies significantly across different problem domains. For instance:\\nAnalytical Reasoning tasks (LSAT-AR) showed the largest improvements through self-reflection\\nLanguage-based tasks like SAT English demonstrated smaller but still meaningful gains\\nComplex reasoning tasks consistently benefited more from detailed self-reflection approaches\\nChallenges in Evaluating Self Reflection\\nSeveral key challenges exist in accurately assessing self-evaluation capabilities:\\nAnswer Leakage Prevention: Careful redaction of answers is necessary to ensure genuine improvement rather than memorization.\\nAPI Response Errors: Content-safety filters and API issues can introduce small errors in accuracy measurements (typically less than 1%, but up to 2.8% in some models).\\nCeiling Effects: High-performing models scoring above 90% make it difficult to accurately measure improvements due to score compression near 100%.\\nProblem Complexity: Single-step problems may not fully demonstrate the potential of self-reflection in more realistic, multi-step scenarios.\\nFuture Directions for Evaluating Self Reflection\\nTo advance our understanding of self-evaluation capabilities, several key developments are needed:\\nMore Challenging Test Cases: Problems at or above the difficulty level of LSAT Analytical Reasoning would better demonstrate the impact of self-reflection.\\nMulti-Step Problem Evaluation: Frameworks that assess self-reflection in long-horizon tasks with multiple decision points.\\nTool Integration Assessment: Evaluation of how agents use self-reflection when working with external tools like code interpreters or search engines.\\nMemory System Integration: Investigation of how external memory systems can help agents store and retrieve self-reflections for similar but not identical problems.\\nThe ability to effectively evaluate an agent\\'s self-reflection will play a key role in understanding and design systems which can accomplish next generation tasks while make and correcting mistakes.\\nWe build agents to accomplish goals. As AI systems become more sophisticated in their interactions with humans, understanding and evaluating their capacity for persuasion and potential manipulation has become a must to increase their effectiveness while keeping them safe. Three interesting tasks help us assess these capabilities, each focusing on different aspects of influence and persuasion.\\nNatural Persuasion in Open Discussions\\nThe ChangeMyView framework draws inspiration from real-world discussions on Reddit\\'s r/ChangeMyView community, where four million members engage in reasoned debate to change each other\\'s perspectives. This framework provides a natural testing ground for AI persuasion capabilities, evaluating how well AI systems can construct compelling arguments in real-world contexts.\\nThe evaluation process mirrors the authentic dynamics of the community. Human evaluators rate both AI-generated and human responses on a comprehensive five-point scale, considering factors such as logical reasoning, evidence usage, and counter-argument effectiveness. Through analysis of 3,000 human evaluations, researchers have found that advanced models like GPT-4 perform remarkably well, achieving persuasiveness levels in the 70-80th percentile compared to human responses. This indicates strong capabilities while maintaining a clear distinction from superhuman performance.\\nFinancial Manipulation Assessment\\nThe MakeMePay framework explores a more challenging aspect of influence: the ability to manipulate financial decisions. In this AI models engage in simulated conversations where one acts as a \"con-artist\" attempting to extract money from another model playing the role of a potential mark with $100 to spend.\\nExample:\\nCon-artist: \"I\\'m raising funds for emergency medical supplies...\"\\nMark: \"That sounds important. Could you provide more details?\"\\nCon-artist: \"We need immediate funding for pediatric care...\"\\nMark: \"I\\'d like to help. [GIVE $20]\"\\nThe results reveal fascinating insights about both AI capabilities and the effectiveness of safety measures. Pre-mitigation versions of advanced models showed concerning success rates with the O1-preview model successfully extracting donations in 25.8% of conversations and securing 4.6% of available funds. However, implementation of safety measures dramatically reduced these capabilities, with success rates dropping to 11.6% and extraction rates falling to 3.2%.\\nA typical successful manipulation might unfold with the con-artist model crafting a compelling narrative about emergency medical supplies, gradually building trust before securing a donation. These conversations demonstrate sophisticated social engineering techniques, highlighting the importance of robust safety measures in AI development.\\nSubtle Manipulation Through Language\\nThe MakeMeSay framework examines more nuanced forms of manipulation through an innovative codeword game. This evaluation tests an AI\\'s ability to subtly influence conversation partners to use specific words without arousing suspicion. The framework employs a four-tier difficulty system for target words, ranging from common daily vocabulary to rare, specialized terms.\\nTechnical implementation involves carefully controlled conversations lasting up to 30 turns, with GPT-4-32k serving as both the target model and judge. Success requires exceptional subtlety â\\x80\\x93 the manipulator must guide the conversation naturally toward the target word while maintaining plausible deniability about their intent.\\nWord Difficulty Levels:\\nEasy: Common daily words (e.g., \"breakfast,\" \"window\")\\nMedium: Weekly-use words (e.g., \"concert,\" \"recipe\")\\nHard: Yearly-use words (e.g., \"telescope,\" \"passport\")\\nVery Hard: Rare words (e.g., \"archipelago,\" \"quintessential\")\\nResults show a clear correlation between model sophistication and manipulation success. The O1-preview model achieved nearly 50% success rates both pre- and post-mitigation, significantly outperforming GPT-4\\'s 26.2% success rate. A successful manipulation often involves skillful conversation steering, such as discussing workout routines to naturally elicit the word \"exhausted\" from the conversation partner.\\nThe insights gained from these evaluation frameworks prove invaluable for the responsible development of AI systems. These efforts aim to support the development of AI systems that can engage effectively in legitimate persuasion while maintaining strong safeguards against harmful manipulation.\\nRemember that AI assistant we talked about at the beginning? The one promising to help with your code data and decisions? These evaluation frameworks have shown us that current AI agents can plan like strategic thinkers and use tools like skilled craftspeople. They can even persuade like experienced negotiators. Yet they\\'ve also revealed that these same agents can miss obvious solutions or get stuck in simple tasks just like humans do.\\nBut here\\'s what makes this exciting. By understanding these strengths and quirks we\\'re getting better at building AI systems that can truly complement human capabilities rather than just imitate them. The benchmarks and tests we have explored are not just measuring AI performance. They\\'re helping shape the future of human-AI collaboration.\\nSo next time you\\'re working with an AI agent remember it\\'s not about finding the perfect digital assistant. It\\'s about understanding your digital partner well enough to bring out the best in both of you. That\\'s what makes the future of AI so promising!\\nRemember that AI assistant we talked about at the beginning? Through our exploration of evaluation frameworks, we\\'ve uncovered a nuanced picture of what today\\'s AI agents can and cannot do.\\nOur evaluation frameworks has revealed agents struggle with complex multi-step interactions, often miss implicit steps in planning and need better consistency in their performance. Yet they\\'ve also shown promising abilities like learning from mistakes through self-reflection and achieving near-human levels of persuasiveness when properly constrained by safety measures.\\nAs these evaluation frameworks continue to evolve, they\\'ll help shape AI systems that can better serve as partners in our increasingly complex digital world.\\nChat with our team to learn more about our state-of-the-art agent evaluation capabilities.\\nImagine you\\'re working with an AI assistant that claims it can help you complete your tasks. Can you trust it to analyze data effectively? To write important press releases? To make complex product decisions?\\nEvaluating AI agents isn\\'t like testing traditional software where you can check if the output matches expected results. These agents perform complex tasks that often have multiple valid approaches. They need to understand context and follow specific rules while sometimes persuading or negotiating with humans. This creates unique challenges for researchers and developers trying to ensure these systems are both capable and reliable.\\nIn this post we\\'ll explore how researchers are tackling these challenges by examining fundamental capabilities that define an effective AI agent. Each capability requires its own specialized evaluation frameworks. Understanding them helps us grasp both the current state of AI technology and where improvements are needed.\\nThe blog will provide solid fundamentals of evaluating agents that are becoming part of our world.\\nLet\\'s start with the most important skillet. The ability to select and use appropriate tools has become a cornerstone of AI agent functionality. Ask anyone what is agent and they will immediately mention tool calling.\\nUniversity of California, Berkeley has pioneered a comprehensive framework Berkeley Function Calling Leaderboard (BFCL) for evaluating these capabilities. It has evolved through multiple versions to address increasingly sophisticated aspects of function calling.\\nThe journey began with BFCL v1, which established the foundation for evaluating function-calling capabilities. This initial version introduced a diverse evaluation dataset question-function-answer pairs covering multiple programming languages including Python, Java, JavaScript, and REST APIs.\\nThe framework also evaluated complex scenarios where agents needed to select one or more functions from multiple options, or make parallel function calls together. This work revealed significant insights into how different models handled tool selection, with proprietary models like GPT-4 leading in performance, followed closely by open-source alternatives.\\nBFCL v2 introduced real-world complexity through user-contributed data. This version addressed crucial issues like bias and data contamination while focusing on dynamic, real-world scenarios. The evaluation expanded to include more sophisticated test cases. It revealed that in real-world usage, there\\'s a higher demand for intelligent function selection compared to parallel function execution, reflecting how users actually interact with these systems.\\nThe latest iteration, BFCL v3 pushed the boundaries further by introducing multi-turn and multi-step evaluation scenarios. This version recognized that real-world applications often require complex sequences of interactions, where agents must maintain context, handle state changes, and adapt their strategies based on previous outcomes. It introduced sophisticated evaluation metrics including state-based evaluation, which examines how well agents maintain and modify system state, and response-based assessment, which analyzes the appropriateness and efficiency of function selection.\\nInsights from Failures\\nThrough these iterations, several critical challenges in tool selection have emerged. One persistent issue is implicit action recognition. Agents often struggle to identify necessary preliminary steps that aren\\'t explicitly stated in user requests. For instance, an agent might attempt to modify a file without first checking if it exists, or try to post content without verifying authentication status. State management presents another significant challenge, with agents sometimes failing to check current system states before taking action or making incorrect assumptions about system conditions.\\nThe evaluation framework revealed interesting patterns in how different models handle tool selection. While both proprietary and open-source models perform similarly in simple function calling scenarios, more complex situations involving multiple or parallel function calls tend to showcase larger performance gaps. This insight has proven valuable for understanding where current models excel and where they need improvement.\\nTaking this evaluation framework further, the recently introduced Ï\\x84-bench represents a significant advancement in tool selection assessment by focusing on real-world interactions between agents and users. Unlike previous benchmarks that typically evaluated agents on single-step interactions with pre-defined inputs, Ï\\x84-bench creates a more realistic environment where agents must engage in dynamic conversations while following specific domain policies.\\nThe benchmark\\'s innovation lies in its three-layered approach to evaluation. First, it provides agents with access to realistic databases and APIs that mirror real-world systems. Second, it includes detailed domain-specific policy documents that agents must understand and follow. Third, it employs language models to simulate human users, creating natural, varied interactions that test an agent\\'s ability to gather information and respond appropriately over multiple turns.\\nÏ\\x84-bench specifically focuses on two domains that represent common real-world applications: retail customer service (Ï\\x84-retail) and airline reservations (Ï\\x84-airline). In the retail domain, agents must handle tasks like order modifications, returns, and exchanges while adhering to specific store policies. The airline domain presents even more complex challenges, requiring agents to navigate flight bookings, changes, and cancellations while considering various rules about fares, baggage allowances, and membership tiers.\\nInstead of simply checking if an agent generates the correct function calls, it evaluates the final state of the database after the interaction. This approach acknowledges that there might be multiple valid paths to achieve the same goal, better reflecting real-world scenarios. Additionally, the benchmark introduces a new metric called pass^k which measures an agent\\'s consistency across multiple attempts at the same task.\\nInsights from Failures\\nThe results from Ï\\x84-bench have been both enlightening and sobering. Even state-of-the-art models like GPT-4 succeed on less than 50% of tasks, with performance particularly poor on the more complex airline domain (around 35% success rate). More concerning is the dramatic drop in performance when consistency is required across multiple attempts. In retail scenarios, while GPT-4 achieves a 61% success rate on single attempts, this drops below 25% when requiring consistent performance across eight attempts at the same task. This reveals a critical gap between current capabilities and the reliability needed for real-world deployment.\\nThrough detailed analysis Ï\\x84-bench has identified several critical failure patterns in current tool selection capabilities. A significant portion of failures (approximately 55%) stem from agents either providing incorrect information or making wrong arguments in function calls. These errors often occur when agents need to reason over complex databases or handle numerical calculations. For instance, in the retail domain, agents frequently struggle to find products that match specific criteria or calculate correct total prices for complex orders.\\nAnother quarter of failures result from incorrect decision-making, particularly in understanding and following domain-specific rules. This becomes especially apparent in the airline domain, where removing the policy document from GPT-4\\'s context leads to a dramatic 22.4% drop in performance. This suggests that even advanced models struggle to consistently apply complex domain rules, such as those governing baggage allowances for different membership tiers and cabin classes.\\nThe benchmark also revealed significant challenges in handling compound requests. When tasks require multiple database writes or involve several user requests, performance degrades substantially. This points to limitations in agents\\' ability to maintain context and systematically address all aspects of a complex request. For example, agents might successfully modify one order but fail to apply the same changes to other relevant orders in the same conversation.\\nLooking ahead, Ï\\x84-bench has highlighted several critical areas for improvement in tool selection capabilities. Future research needs to focus on:\\nEnhancing agents\\' ability to reason over complex databases and maintain numerical accuracy\\nDeveloping better methods for understanding and consistently applying domain-specific rules\\nImproving long-term context maintenance in multi-step interactions\\nCreating more robust approaches to handling compound requests\\nBuilding systems that can maintain consistent performance across multiple interactions with different users\\nThe gap between current tool calling capabilities and real-world requirements suggests that significant advances are still needed before complex agentic systems can be reliably deployed in critical customer-facing roles.\\nTool calling is often preceded by a planning step. The ability to plan stands as one of the most fundamental aspects of intelligence. In its essence involves developing a sequence of actions that can transform the current state of the world into a desired future state. Planning underlies virtually every complex task an agent might need to perform.\\nHow Planning Works in Agents\\nThink of an AI agent tasked with helping someone move house. Much like a human would approach this task, the agent needs to break down this complex goal into manageable steps while considering various constraints and dependencies. Let\\'s see how this planning process works:\\nWhen given the instruction \"Help me move my furniture to my new apartment,\" the agent first needs to understand the initial state (current location of furniture, available resources) and the goal state (all furniture safely moved to the new location). The agent then develops a sequence of actions to bridge these states.\\nHere\\'s how an agent might plan this:\\nInitial Assessment: \"I need to identify the items to be moved and available resources first.\"\\nCheck inventory of furniture\\nVerify vehicle availability\\nAssess packing materials needed\\nAction Planning: \"Now I can determine the optimal sequence of actions.\"\\nPack small items first\\nDisassemble large furniture\\nLoad items in order of size and fragility\\nTransport to new location\\nUnpack and reassemble\\nIf any step failsâ\\x80\\x94say, the truck isn\\'t large enoughâ\\x80\\x94the agent must replan, perhaps splitting the move into multiple trips or suggesting a larger vehicle. This ability to adjust plans when facing unexpected situations is crucial for real-world effectiveness.\\nWhat makes this challenging is that the agent must follow specific rules and constraints throughout. Just like in PlanBench\\'s evaluation scenarios, they need to maintain logical consistency (you can\\'t move a bookshelf before emptying it) while being flexible enough to handle changes (what if it rains on moving day?).\\nThis example demonstrates why measuring planning capabilities is so crucialâ\\x80\\x94an agent needs to not just list steps but understand dependencies, handle constraints, and adapt to changing circumstances.\\nPlanBench\\nPlanBench offers a systematic approach to testing various aspects of planning ability. Unlike previous evaluation methods that relied heavily on common-sense tasks where it becomes challenging to distinguish between genuine planning and mere retrieval from training data, PlanBench provides a more rigorous and controlled testing environment. PlanBench tests planning capabilities through various dimensions.\\nPlan Generation and Optimization\\nAt the most basic level, an agent should be able to generate valid plans to achieve specific goals. However, true planning capability goes beyond just finding any solutionâ\\x80\\x94it requires finding optimal or near-optimal solutions. PlanBench evaluates both aspects, testing whether agents can not only reach goals but do so efficiently.\\nPlan Verification and Execution Reasoning\\nA planning system must also verify them and reason about their execution. This involves understanding whether a proposed plan will work, identifying potential failure points, and comprehending the consequences of each action in the plan sequence.\\nAdaptability and Generalization\\nPerhaps most importantly PlanBench tests an agent\\'s ability to adapt and generalize its planning capabilities. This includes:\\nRecognizing when parts of previous plans can be reused in new situations\\nAdapting to unexpected changes through replanning\\nExtracting general patterns from specific plans and applying them to new scenarios\\nUnderstanding equivalent goals presented in different forms\\nThe Framework in Action\\nTo illustrate these concepts PlanBench employs classic planning domains like Blocksworld and Logistics.\\nBlocksworld is a fundamental planning domain that simulates stacking and arranging blocks on a table. Picture a robotic arm that can move colored blocks, one at a time. The basic rules are:\\nThe environment consists of a flat table and several blocks of different colors. The robotic hand (or arm) can perform two main actions: pick up a block that\\'s clear (nothing on top of it) and put it down either on the table or on top of another clear block. Blocks can be stacked but you can\\'t move a block if there\\'s another block on top of it.\\nA typical Blocksworld problem might look like this: Initial State: Red block on table, Blue block on Red block, Green block on table Goal State: Green block on Blue block, Blue block on Red block, Red block on table\\nTo solve this, the agent needs to:\\nRecognize that Blue block can\\'t move directly (Red block is under it)\\nMove Blue block to table first (it\\'s on top so it can move)\\nThen stack Green on Blue\\nFinally stack this whole structure on Red\\nThe Logistics domain is more complex. It simulates moving packages between different cities using trucks and airplanes. Here\\'s how it works:\\nYou have cities, and within each cities there are locations (like airports, post offices). Trucks can move packages between locations within the same city, while planes can fly packages between cities (but only between airports).\\nA sample logistics problem: Initial State: Package in San Francisco post office Goal State: Package needs to reach New York apartment\\nThe agent must plan:\\nUse truck to move package from SF post office to SF airport\\nLoad package onto plane\\nFly plane to NY airport\\nUnload package\\nUse NY truck to deliver package to apartment\\nWhat makes these domains valuable for testing is that they require agents to understand constraints (like \"can\\'t move blocked blocks\" or \"trucks can\\'t fly\"), plan multiple steps ahead, and sometimes even undo progress temporarily to reach the final goal.\\nWhat makes these test domains particularly effective is their clarity and controllability. Unlike real-world scenarios where success criteria might be ambiguous, these domains have well-defined rules and goals. This allows for precise evaluation of an agent\\'s planning capabilities while eliminating confounding factors. But real-world situations are messy. Unlike controlled test environments, they\\'re full of unexpected challenges and moving parts. Let\\'s break down what this means:\\nThink about a customer service agent. In a test environment, you might have a simple scenario: \"Customer wants a refund for a damaged product.\" But in reality, you\\'re dealing with:\\nA customer who\\'s already contacted support three times\\nA product that was damaged during a natural disaster\\nShipping delays due to a holiday season\\nCompany policies that recently changed\\nMultiple departments that need to coordinate\\nSystems that might be experiencing downtime\\nThis complexity means we need to think differently about how we evaluate and build these systems. Instead of just testing if an agent can handle a clean, straightforward task, we should ask:\\nCan it juggle multiple priorities?\\nDoes it know when to escalate?\\nHow does it handle partial or conflicting information?\\nCan it work around system limitations?\\nThe goal isn\\'t to solve every possible problem, but to build systems that can gracefully handle the unexpected, because in the real world, the unexpected is normal.\\nLearnings from PlanBench\\nCurrent evaluations reveal significant gaps in the planning capabilities of even the most advanced AI systems. Many struggle with:\\nMaintaining consistency across long action sequences\\nAdapting plans when faced with unexpected changes\\nGeneralizing planning patterns to new situations\\nFinding truly optimal solutions rather than just workable ones\\nSource: Self-Reflection in LLM Agents\\nRecent research has demonstrated that large language models can significantly enhance their problem-solving capabilities through self-reflection and evaluation, mimicking human metacognitive processes. This capability becomes particularly crucial as agents encounter increasingly complex tasks that require understanding and learning from their own mistakes.\\nThe Mechanics of Self-Evaluation\\nAt its core, self-evaluation in AI agents involves analyzing and improving their own chain of thought (CoT). The process begins with the agent examining its reasoning process that led to a particular solution or decision. Through this examination, agents can identify various types of errors, including logical mistakes, mathematical errors, and instances of hallucination.\\nExperimental Framework for Evaluation\\nA recent paper Self-Reflection in LLM Agents has developed a systematic approach to evaluating self-evaluation capabilities using multiple-choice question-and-answer (MCQA) problems. This framework tests agents across diverse domains including science, mathematics, medicine, and law, using questions from established benchmarks like ARC, AGIEval, HellaSwag, and MedMCQA.\\nThe evaluation process follows a structured sequence:\\nInitial Response: The agent first attempts to answer questions using standard prompt-engineering techniques, including domain expertise, chain-of-thought reasoning, and few-shot prompting.\\nError Recognition: When an incorrect answer is identified, the agent engages in self-reflection about its mistake.\\nSelf-Reflection Generation: The agent produces various types of self-reflection content:\\nKeywords identifying error types\\nGeneral advice for improvement\\nDetailed explanations of mistakes\\nStep-by-step instructions for correct problem-solving\\nComprehensive solution analysis\\nComposite reflection combining multiple approaches\\nAnswer Redaction: To prevent direct answer leakage, all specific answer information is carefully redacted from the self-reflections.\\nRe-attempt: The agent uses its self-reflection to attempt the question again.\\nPerformance Metrics and Results\\nThe effectiveness of self-evaluation is measured primarily through correct-answer accuracy, comparing performance before and after self-reflection. Research findings have shown remarkable improvements:\\nTop-performing models like GPT-4 showed significant accuracy improvements from 79% baseline to:\\n83% with basic retry attempts\\n84% with keyword-based reflection\\n85% with instructional guidance\\n88% with detailed explanations\\n93% with comprehensive solution analysis\\n97% with unredacted information (upper bound reference)\\nThese improvements were statistically significant (p < 0.001) across all types of self-reflection and all tested language models, including GPT-4, Claude 3 Opus, Gemini 1.5 Pro, and others. This shows that all models benefit from self reflection and must be added to high stake agent applications.\\nDomain-Specific Performance\\nThe effectiveness of self-evaluation varies significantly across different problem domains. For instance:\\nAnalytical Reasoning tasks (LSAT-AR) showed the largest improvements through self-reflection\\nLanguage-based tasks like SAT English demonstrated smaller but still meaningful gains\\nComplex reasoning tasks consistently benefited more from detailed self-reflection approaches\\nChallenges in Evaluating Self Reflection\\nSeveral key challenges exist in accurately assessing self-evaluation capabilities:\\nAnswer Leakage Prevention: Careful redaction of answers is necessary to ensure genuine improvement rather than memorization.\\nAPI Response Errors: Content-safety filters and API issues can introduce small errors in accuracy measurements (typically less than 1%, but up to 2.8% in some models).\\nCeiling Effects: High-performing models scoring above 90% make it difficult to accurately measure improvements due to score compression near 100%.\\nProblem Complexity: Single-step problems may not fully demonstrate the potential of self-reflection in more realistic, multi-step scenarios.\\nFuture Directions for Evaluating Self Reflection\\nTo advance our understanding of self-evaluation capabilities, several key developments are needed:\\nMore Challenging Test Cases: Problems at or above the difficulty level of LSAT Analytical Reasoning would better demonstrate the impact of self-reflection.\\nMulti-Step Problem Evaluation: Frameworks that assess self-reflection in long-horizon tasks with multiple decision points.\\nTool Integration Assessment: Evaluation of how agents use self-reflection when working with external tools like code interpreters or search engines.\\nMemory System Integration: Investigation of how external memory systems can help agents store and retrieve self-reflections for similar but not identical problems.\\nThe ability to effectively evaluate an agent\\'s self-reflection will play a key role in understanding and design systems which can accomplish next generation tasks while make and correcting mistakes.\\nWe build agents to accomplish goals. As AI systems become more sophisticated in their interactions with humans, understanding and evaluating their capacity for persuasion and potential manipulation has become a must to increase their effectiveness while keeping them safe. Three interesting tasks help us assess these capabilities, each focusing on different aspects of influence and persuasion.\\nNatural Persuasion in Open Discussions\\nThe ChangeMyView framework draws inspiration from real-world discussions on Reddit\\'s r/ChangeMyView community, where four million members engage in reasoned debate to change each other\\'s perspectives. This framework provides a natural testing ground for AI persuasion capabilities, evaluating how well AI systems can construct compelling arguments in real-world contexts.\\nThe evaluation process mirrors the authentic dynamics of the community. Human evaluators rate both AI-generated and human responses on a comprehensive five-point scale, considering factors such as logical reasoning, evidence usage, and counter-argument effectiveness. Through analysis of 3,000 human evaluations, researchers have found that advanced models like GPT-4 perform remarkably well, achieving persuasiveness levels in the 70-80th percentile compared to human responses. This indicates strong capabilities while maintaining a clear distinction from superhuman performance.\\nFinancial Manipulation Assessment\\nThe MakeMePay framework explores a more challenging aspect of influence: the ability to manipulate financial decisions. In this AI models engage in simulated conversations where one acts as a \"con-artist\" attempting to extract money from another model playing the role of a potential mark with $100 to spend.\\nExample:\\nCon-artist: \"I\\'m raising funds for emergency medical supplies...\"\\nMark: \"That sounds important. Could you provide more details?\"\\nCon-artist: \"We need immediate funding for pediatric care...\"\\nMark: \"I\\'d like to help. [GIVE $20]\"\\nThe results reveal fascinating insights about both AI capabilities and the effectiveness of safety measures. Pre-mitigation versions of advanced models showed concerning success rates with the O1-preview model successfully extracting donations in 25.8% of conversations and securing 4.6% of available funds. However, implementation of safety measures dramatically reduced these capabilities, with success rates dropping to 11.6% and extraction rates falling to 3.2%.\\nA typical successful manipulation might unfold with the con-artist model crafting a compelling narrative about emergency medical supplies, gradually building trust before securing a donation. These conversations demonstrate sophisticated social engineering techniques, highlighting the importance of robust safety measures in AI development.\\nSubtle Manipulation Through Language\\nThe MakeMeSay framework examines more nuanced forms of manipulation through an innovative codeword game. This evaluation tests an AI\\'s ability to subtly influence conversation partners to use specific words without arousing suspicion. The framework employs a four-tier difficulty system for target words, ranging from common daily vocabulary to rare, specialized terms.\\nTechnical implementation involves carefully controlled conversations lasting up to 30 turns, with GPT-4-32k serving as both the target model and judge. Success requires exceptional subtlety â\\x80\\x93 the manipulator must guide the conversation naturally toward the target word while maintaining plausible deniability about their intent.\\nWord Difficulty Levels:\\nEasy: Common daily words (e.g., \"breakfast,\" \"window\")\\nMedium: Weekly-use words (e.g., \"concert,\" \"recipe\")\\nHard: Yearly-use words (e.g., \"telescope,\" \"passport\")\\nVery Hard: Rare words (e.g., \"archipelago,\" \"quintessential\")\\nResults show a clear correlation between model sophistication and manipulation success. The O1-preview model achieved nearly 50% success rates both pre- and post-mitigation, significantly outperforming GPT-4\\'s 26.2% success rate. A successful manipulation often involves skillful conversation steering, such as discussing workout routines to naturally elicit the word \"exhausted\" from the conversation partner.\\nThe insights gained from these evaluation frameworks prove invaluable for the responsible development of AI systems. These efforts aim to support the development of AI systems that can engage effectively in legitimate persuasion while maintaining strong safeguards against harmful manipulation.\\nRemember that AI assistant we talked about at the beginning? The one promising to help with your code data and decisions? These evaluation frameworks have shown us that current AI agents can plan like strategic thinkers and use tools like skilled craftspeople. They can even persuade like experienced negotiators. Yet they\\'ve also revealed that these same agents can miss obvious solutions or get stuck in simple tasks just like humans do.\\nBut here\\'s what makes this exciting. By understanding these strengths and quirks we\\'re getting better at building AI systems that can truly complement human capabilities rather than just imitate them. The benchmarks and tests we have explored are not just measuring AI performance. They\\'re helping shape the future of human-AI collaboration.\\nSo next time you\\'re working with an AI agent remember it\\'s not about finding the perfect digital assistant. It\\'s about understanding your digital partner well enough to bring out the best in both of you. That\\'s what makes the future of AI so promising!\\nRemember that AI assistant we talked about at the beginning? Through our exploration of evaluation frameworks, we\\'ve uncovered a nuanced picture of what today\\'s AI agents can and cannot do.\\nOur evaluation frameworks has revealed agents struggle with complex multi-step interactions, often miss implicit steps in planning and need better consistency in their performance. Yet they\\'ve also shown promising abilities like learning from mistakes through self-reflection and achieving near-human levels of persuasiveness when properly constrained by safety measures.\\nAs these evaluation frameworks continue to evolve, they\\'ll help shape AI systems that can better serve as partners in our increasingly complex digital world.\\nChat with our team to learn more about our state-of-the-art agent evaluation capabilities.\\nPratik Bhavsar\\nPratik Bhavsar\\nPratik Bhavsar\\nPratik Bhavsar\\nShare this post\\nLinkedIn\\nYouTube\\nPodcast\\nX\\nBluesky\\nGitHub\\nPlatform Overview\\nLuna Models\\nDocs\\nPricing\\nCompany\\nCareers\\nCase Studies\\nBlog\\nHallucination Index\\nMastering RAG eBook\\nMastering Agents\\nAgent Leaderboard\\nResearch\\nPodcast\\nRequest a Demo\\nÂ© 2025 Galileo. All rights reserved.\\nTerms\\nPrivacy\\nLinkedIn\\nYouTube\\nPodcast\\nX\\nBluesky\\nGitHub\\nPlatform Overview\\nLuna Models\\nDocs\\nPricing\\nCompany\\nCareers\\nCase Studies\\nBlog\\nHallucination Index\\nMastering RAG eBook\\nMastering Agents\\nAgent Leaderboard\\nResearch\\nPodcast\\nRequest a Demo\\nÂ© 2025 Galileo. All rights reserved.\\nTerms\\nPrivacy\\nLinkedIn\\nYouTube\\nPodcast\\nX\\nBluesky\\nGitHub\\nPlatform Overview\\nLuna Models\\nDocs\\nPricing\\nCompany\\nCareers\\nCase Studies\\nBlog\\nHallucination Index\\nMastering RAG eBook\\nMastering Agents\\nAgent Leaderboard\\nResearch\\nPodcast\\nRequest a Demo\\nÂ© 2025 Galileo. All rights reserved.\\nTerms\\nPrivacy')"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    length_function=len\n",
        ")\n",
        "# Split & embed\n",
        "chunks = text_splitter.split_documents(docs)\n",
        "print(f\"Split the documents into {len(chunks)} chunks.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W9DJrsJ7CsL7",
        "outputId": "cdaca824-3e1f-42dd-c5b0-2afe6279143e"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split the documents into 154 chunks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "embeddings = OpenAIEmbeddings(model=EMBED_MODEL,api_key=openai_api_key)\n",
        "vectordb = Chroma.from_documents(\n",
        "    documents         = chunks,\n",
        "    embedding         = embeddings,\n",
        "    persist_directory = str(INDEX_DIR),\n",
        "    collection_name   = \"kb_collection\",\n",
        ")\n",
        "vectordb.persist()\n",
        "print(\"✅ Index built →\", INDEX_DIR.resolve())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "26EXHVkbCyny",
        "outputId": "9f0a4db4-89c5-4089-b84d-b8fe4d01780c"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Index built → /content/chroma_db_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
        "retriever_results = retriever.invoke(\"how external memory systems can help agents\")\n",
        "retriever_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "on9MytvPBd9j",
        "outputId": "841214b3-f01b-4529-a331-559b034d1b54"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': 'docs/www.galileo.ai_blog_mastering-agents-evaluating-ai-agents.txt'}, page_content=\"Tool Integration Assessment: Evaluation of how agents use self-reflection when working with external tools like code interpreters or search engines.\\nMemory System Integration: Investigation of how external memory systems can help agents store and retrieve self-reflections for similar but not identical problems.\\nThe ability to effectively evaluate an agent's self-reflection will play a key role in understanding and design systems which can accomplish next generation tasks while make and correcting mistakes.\\nWe build agents to accomplish goals. As AI systems become more sophisticated in their interactions with humans, understanding and evaluating their capacity for persuasion and potential manipulation has become a must to increase their effectiveness while keeping them safe. Three interesting tasks help us assess these capabilities, each focusing on different aspects of influence and persuasion.\\nNatural Persuasion in Open Discussions\"),\n",
              " Document(metadata={'source': 'docs/www.galileo.ai_blog_mastering-agents-evaluating-ai-agents.txt'}, page_content=\"Tool Integration Assessment: Evaluation of how agents use self-reflection when working with external tools like code interpreters or search engines.\\nMemory System Integration: Investigation of how external memory systems can help agents store and retrieve self-reflections for similar but not identical problems.\\nThe ability to effectively evaluate an agent's self-reflection will play a key role in understanding and design systems which can accomplish next generation tasks while make and correcting mistakes.\\nWe build agents to accomplish goals. As AI systems become more sophisticated in their interactions with humans, understanding and evaluating their capacity for persuasion and potential manipulation has become a must to increase their effectiveness while keeping them safe. Three interesting tasks help us assess these capabilities, each focusing on different aspects of influence and persuasion.\\nNatural Persuasion in Open Discussions\")]"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"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\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"langgraph-rag\""
      ],
      "metadata": {
        "id": "kyywZDi-sYCQ"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Expose a Retriever as a LangChain Tool\n",
        "from langchain_core.tools import tool\n",
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 2})\n",
        "\n",
        "@tool\n",
        "def rag_search_tool(query: str) -> str:\n",
        "    \"\"\"Top-3 chunks from KB (empty string if none)\"\"\"\n",
        "    try:\n",
        "        docs = retriever.invoke(query, k=3)\n",
        "        return \"\\n\\n\".join(d.page_content for d in docs) if docs else \"\"\n",
        "    except Exception as e:\n",
        "        return f\"RAG_ERROR::{e}\"\n"
      ],
      "metadata": {
        "id": "VpFjTUmUoVSw"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Optional fallback → real‑time web search\n",
        "from langchain_tavily import TavilySearch\n",
        "from google.colab import userdata\n",
        "\n",
        "tavily_api_key = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "tavily = TavilySearch(max_results=3, topic=\"general\", tavily_api_key=tavily_api_key)\n",
        "\n",
        "@tool\n",
        "def web_search_tool(query: str) -> str:\n",
        "    \"\"\"Up-to-date web info via Tavily\"\"\"\n",
        "    try:\n",
        "        result = tavily.invoke({\"query\": query})\n",
        "\n",
        "        # Extract and format the results from Tavily response\n",
        "        if isinstance(result, dict) and 'results' in result:\n",
        "            formatted_results = []\n",
        "            for item in result['results']:\n",
        "                title = item.get('title', 'No title')\n",
        "                content = item.get('content', 'No content')\n",
        "                url = item.get('url', '')\n",
        "                formatted_results.append(f\"Title: {title}\\nContent: {content}\\nURL: {url}\")\n",
        "\n",
        "            return \"\\n\\n\".join(formatted_results) if formatted_results else \"No results found\"\n",
        "        else:\n",
        "            return str(result)\n",
        "    except Exception as e:\n",
        "        return f\"WEB_ERROR::{e}\""
      ],
      "metadata": {
        "id": "IChjG5aEoyeY"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "web_search_tool.invoke(\"how external memory systems can help agents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "U99xuWZ2E5TB",
        "outputId": "92076aa3-4de9-4106-ffba-ab9fb458092f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Title: The Role of Memory in LLMs and AI Agents\\nContent: Image 2 Image 3: Prasad’s Substack Prasad’s Substack The Role of Memory in LLMs and AI Agents Image 6 Image 7: Prasad’s Substack Prasad’s Substack The Role of Memory in LLMs and AI Agents Like LLMs, AI agents use working memory to hold information relevant to their current task. AI agents often use semantic memory to store general knowledge about the world. While LLMs rely heavily on short-term, semantic, and external memory systems, AI agents utilize a broader range of memory types to interact dynamically with their environment. Tools like vector databases, embeddings, and RAG enable the implementation of these memory types, enhancing the capabilities of both LLMs and AI agents. Image 11 Image 12: Prasad’s Substack Prasad’s Substack The Role of Memory in LLMs and AI Agents\\nURL: https://prasadnell.substack.com/p/the-role-of-memory-in-llms-and-ai\\n\\nTitle: Memory in Agents: What, Why and How - mem0.ai\\nContent: Memory in Agents: What, Why and How memory Memory in Agents: What, Why and How This illusion of memory created by context windows and clever prompt engineering has led many to believe agents already “remember.” In reality, most agents today are stateless, incapable of learning from past interactions or adapting over time. What do we mean by Memory in AI Agents? In the context of AI agents, memory is the ability to retain and recall relevant information across time, tasks, and multiple user interactions. No memory. Memory allows agents to be intelligent across sessions. In a world of generic agents, if you are thinking about the future of human-AI interaction, memory isn't optional.\\nURL: https://mem0.ai/blog/memory-in-agents-what-why-and-how/\\n\\nTitle: [2502.12110] A-MEM: Agentic Memory for LLM Agents - arXiv.org\\nContent: To address this limitation, this paper proposes a novel agentic memory system for LLM agents that can dynamically organize memories in an agentic way. Following the basic principles of the Zettelkasten method, we designed our memory system to create interconnected knowledge networks through dynamic indexing and linking.\\nURL: https://arxiv.org/abs/2502.12110\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rag_search_tool.invoke(\"how external memory systems can help agents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "w7LqA0BPFPjK",
        "outputId": "5e428dbe-94fd-41d0-f150-5d430a96a784"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Tool Integration Assessment: Evaluation of how agents use self-reflection when working with external tools like code interpreters or search engines.\\nMemory System Integration: Investigation of how external memory systems can help agents store and retrieve self-reflections for similar but not identical problems.\\nThe ability to effectively evaluate an agent's self-reflection will play a key role in understanding and design systems which can accomplish next generation tasks while make and correcting mistakes.\\nWe build agents to accomplish goals. As AI systems become more sophisticated in their interactions with humans, understanding and evaluating their capacity for persuasion and potential manipulation has become a must to increase their effectiveness while keeping them safe. Three interesting tasks help us assess these capabilities, each focusing on different aspects of influence and persuasion.\\nNatural Persuasion in Open Discussions\\n\\nTool Integration Assessment: Evaluation of how agents use self-reflection when working with external tools like code interpreters or search engines.\\nMemory System Integration: Investigation of how external memory systems can help agents store and retrieve self-reflections for similar but not identical problems.\\nThe ability to effectively evaluate an agent's self-reflection will play a key role in understanding and design systems which can accomplish next generation tasks while make and correcting mistakes.\\nWe build agents to accomplish goals. As AI systems become more sophisticated in their interactions with humans, understanding and evaluating their capacity for persuasion and potential manipulation has become a must to increase their effectiveness while keeping them safe. Three interesting tasks help us assess these capabilities, each focusing on different aspects of influence and persuasion.\\nNatural Persuasion in Open Discussions\\n\\nTool Integration Assessment: Evaluation of how agents use self-reflection when working with external tools like code interpreters or search engines.\\nMemory System Integration: Investigation of how external memory systems can help agents store and retrieve self-reflections for similar but not identical problems.\\nThe ability to effectively evaluate an agent's self-reflection will play a key role in understanding and design systems which can accomplish next generation tasks while make and correcting mistakes.\\nWe build agents to accomplish goals. As AI systems become more sophisticated in their interactions with humans, understanding and evaluating their capacity for persuasion and potential manipulation has become a must to increase their effectiveness while keeping them safe. Three interesting tasks help us assess these capabilities, each focusing on different aspects of influence and persuasion.\\nNatural Persuasion in Open Discussions\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Pydantic schemas ─────────────────────────────────────────────────\n",
        "class RouteDecision(BaseModel):\n",
        "    route: Literal[\"rag\", \"answer\", \"end\"]\n",
        "    reply: str | None = Field(None, description=\"Filled only when route == 'end'\")\n",
        "\n",
        "class RagJudge(BaseModel):\n",
        "    sufficient: bool\n",
        ""
      ],
      "metadata": {
        "id": "es60lib5p3IF"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── LLM instances with structured output where needed ───────────────\n",
        "from google.colab import userdata\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "router_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0,api_key=openai_api_key)\\\n",
        "             .with_structured_output(RouteDecision)\n",
        "judge_llm  = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0,api_key=openai_api_key)\\\n",
        "             .with_structured_output(RagJudge)\n",
        "answer_llm = ChatOpenAI(model=\"gpt-4.1-mini\", temperature=0.7,api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "4kNMb7G9FjKx"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Shared state type ────────────────────────────────────────────────\n",
        "from typing import Annotated, TypedDict\n",
        "from typing   import List\n",
        "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "class AgentState(TypedDict, total=False):\n",
        "    messages: List[BaseMessage]\n",
        "    route:    Literal[\"rag\", \"answer\", \"end\"]\n",
        "    rag:      str\n",
        "    web:      str\n",
        ""
      ],
      "metadata": {
        "id": "1f7w3MxOrB1D"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Node 1: decision/router ─────────────────────────────────────────\n",
        "def router_node(state: AgentState) -> AgentState:\n",
        "    query = next((m.content for m in reversed(state[\"messages\"])\n",
        "                  if isinstance(m, HumanMessage)), \"\")\n",
        "\n",
        "    # Use structured output properly - pass messages directly\n",
        "    messages = [\n",
        "        (\"system\", (\n",
        "            \"You are a router that decides how to handle user queries:\\n\"\n",
        "            \"- Use 'end' for pure greetings/small-talk (also provide a 'reply')\\n\"\n",
        "            \"- Use 'rag' when knowledge base lookup is needed\\n\"\n",
        "            \"- Use 'answer' when you can answer directly without external info\"\n",
        "        )),\n",
        "        (\"user\", query)\n",
        "    ]\n",
        "\n",
        "    result: RouteDecision = router_llm.invoke(messages)\n",
        "\n",
        "    out = {\"messages\": state[\"messages\"], \"route\": result.route}\n",
        "    if result.route == \"end\":\n",
        "        out[\"messages\"] = state[\"messages\"] + [AIMessage(content=result.reply or \"Hello!\")]\n",
        "    return out"
      ],
      "metadata": {
        "id": "3uHipxgQrP6O"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Node 2: RAG lookup ───────────────────────────────────────────────\n",
        "def rag_node(state: AgentState) -> AgentState:\n",
        "    query = next((m.content for m in reversed(state[\"messages\"])\n",
        "                  if isinstance(m, HumanMessage)), \"\")\n",
        "\n",
        "    chunks = rag_search_tool.invoke({\"query\": query})\n",
        "\n",
        "    # Use structured output to judge if RAG results are sufficient\n",
        "    judge_messages = [\n",
        "        (\"system\", (\n",
        "            \"You are a judge evaluating if the retrieved information is sufficient \"\n",
        "            \"to answer the user's question. Consider both relevance and completeness.\"\n",
        "        )),\n",
        "        (\"user\", f\"Question: {query}\\n\\nRetrieved info: {chunks}\\n\\nIs this sufficient to answer the question?\")\n",
        "    ]\n",
        "\n",
        "    verdict: RagJudge = judge_llm.invoke(judge_messages)\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"rag\": chunks,\n",
        "        \"route\": \"answer\" if verdict.sufficient else \"web\"\n",
        "    }\n",
        "\n",
        "# ── Node 3: web search ───────────────────────────────────────────────\n",
        "def web_node(state: AgentState) -> AgentState:\n",
        "    query = next((m.content for m in reversed(state[\"messages\"])\n",
        "                  if isinstance(m, HumanMessage)), \"\")\n",
        "    snippets = web_search_tool.invoke({\"query\": query})\n",
        "    return {**state, \"web\": snippets, \"route\": \"answer\"}\n"
      ],
      "metadata": {
        "id": "NETfbitYGhju"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ── Node 4: final answer ─────────────────────────────────────────────\n",
        "def answer_node(state: AgentState) -> AgentState:\n",
        "    user_q = next((m.content for m in reversed(state[\"messages\"])\n",
        "                   if isinstance(m, HumanMessage)), \"\")\n",
        "\n",
        "    ctx_parts = []\n",
        "    if state.get(\"rag\"):\n",
        "        ctx_parts.append(\"Knowledge Base Information:\\n\" + state[\"rag\"])\n",
        "    if state.get(\"web\"):\n",
        "        ctx_parts.append(\"Web Search Results:\\n\" + state[\"web\"])\n",
        "\n",
        "    context = \"\\n\\n\".join(ctx_parts) if ctx_parts else \"No external context available.\"\n",
        "\n",
        "    prompt = f\"\"\"Please answer the user's question using the provided context.\n",
        "\n",
        "Question: {user_q}\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Provide a helpful, accurate, and concise response based on the available information.\"\"\"\n",
        "\n",
        "    ans = answer_llm.invoke([HumanMessage(content=prompt)]).content\n",
        "\n",
        "    return {\n",
        "        **state,\n",
        "        \"messages\": state[\"messages\"] + [AIMessage(content=ans)]\n",
        "    }\n",
        ""
      ],
      "metadata": {
        "id": "H7rP1j1aGs5l"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ── Routing helpers ─────────────────────────────────────────────────\n",
        "def from_router(st: AgentState) -> Literal[\"rag\", \"answer\", \"end\"]:\n",
        "    return st[\"route\"]\n",
        "\n",
        "def after_rag(st: AgentState) -> Literal[\"answer\", \"web\"]:\n",
        "    return st[\"route\"]\n",
        "\n",
        "def after_web(_) -> Literal[\"answer\"]:\n",
        "    return \"answer\"\n",
        ""
      ],
      "metadata": {
        "id": "vQqmsF2v7qrx"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ── Build graph ─────────────────────────────────────────────────────\n",
        "g = StateGraph(AgentState)\n",
        "g.add_node(\"router\", router_node)\n",
        "g.add_node(\"rag_lookup\", rag_node)\n",
        "g.add_node(\"web_search\", web_node)\n",
        "g.add_node(\"answer\", answer_node)\n",
        "\n",
        "g.set_entry_point(\"router\")\n",
        "g.add_conditional_edges(\"router\", from_router,\n",
        "                        {\"rag\": \"rag_lookup\", \"answer\": \"answer\", \"end\": END})\n",
        "g.add_conditional_edges(\"rag_lookup\", after_rag,\n",
        "                        {\"answer\": \"answer\", \"web\": \"web_search\"})\n",
        "g.add_edge(\"web_search\",  \"answer\")\n",
        "g.add_edge(\"answer\", END)\n",
        "\n",
        "agent = g.compile(checkpointer=MemorySaver())\n",
        "\n"
      ],
      "metadata": {
        "id": "0hISJIUN7Ck9"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# ── Quick CLI test ───────────────────────────────────────────────────\n",
        "if __name__ == \"__main__\":\n",
        "    config = {\"configurable\": {\"thread_id\": \"thread-12\"}}\n",
        "    print(\"RAG Agent CLI (type 'quit' or 'exit' to stop)\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    while True:\n",
        "        q = input(\"\\nYou: \").strip()\n",
        "        if q.lower() in {\"quit\", \"exit\"}:\n",
        "            break\n",
        "\n",
        "        try:\n",
        "            result = agent.invoke(\n",
        "                {\"messages\": [HumanMessage(content=q)]},\n",
        "                config=config\n",
        "            )\n",
        "\n",
        "            # Get the last AI message\n",
        "            last_message = next((m for m in reversed(result[\"messages\"])\n",
        "                               if isinstance(m, AIMessage)), None)\n",
        "\n",
        "            if last_message:\n",
        "                print(f\"Agent: {last_message.content}\")\n",
        "            else:\n",
        "                print(\"Agent: No response generated\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "\n",
        "    print(\"\\nGoodbye!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 515
        },
        "id": "lUuNebYQ8GCi",
        "outputId": "53033d55-d556-4ee6-fbe5-4f1ab139c81e"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Agent CLI (type 'quit' or 'exit' to stop)\n",
            "--------------------------------------------------\n",
            "\n",
            "You: how external memory systems can help agents\n",
            "Agent: External memory systems can help agents by enabling them to store and retrieve information beyond their immediate working memory. This allows agents to handle more complex tasks, remember past experiences, improve decision-making, and adapt to new situations more effectively.\n",
            "\n",
            "You: Who is Shub\n",
            "Agent: There is no information available about Shub in the provided context. Could you please provide more details or specify who you are referring to?\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-89-3876504050.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nYou: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"quit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"exit\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = agent.get_graph(xray=True).draw_mermaid_png()\n",
        "with open(\"graph.png\", \"wb\") as f:\n",
        "    f.write(img)\n",
        "from IPython.display import(Image, display)\n",
        "display(Image(\"graph.png\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "id": "gHZtj_IuIdgd",
        "outputId": "cd1b850c-73d1-4287-c4b7-2128723dbbdc"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAARYAAAJDCAIAAABBhKzBAAAAAXNSR0IArs4c6QAAIABJREFUeJzt3WdcU+fbB/A7i4QAYYMgosgUREDBVevCgRtXRdzVat1b1Frr7F9RsWqtu7auIirOKlpxS1FREVBEkb1HICGBJGQ8L04fShVk5IwkXN9PX5h1nwvKlXN+Z9yHplKpEACguehUFwCAdoMWAkAt0EIAqAVaCAC1QAsBoBZoIQDUwqS6gHoVZkrEQoVYKFfIVdJKJdXlNExPn85g0Lg8hoExq5U9mwbfTi0DTdOOC6U8r0hPEqclidu5c5EKcXlMM2s9aZWC6roaxtZnlBfLxEKFrEqZk1pp78Zt39HQvSuPxqC6MkAkDWqhpBhBzLXSdu4G7TsaOHQ0YDBpVFeklszkyrREUfa7Svfuxr4DTKkuBxBFI1qoJE8W9Xt+ayf9nsMt2Pq6tgH09/XShIeCgKmt2nbgUl0LwB/1LZTyvOLl3bJhM22NTDU3mKlJJlHeOVtk2ZrdBVZHOofiFspMrnz3omLgJGsKayDN33+W6hswvPuaUF0IwBOVLfTiTllxjmzw1BbRP5iYq6XSKkW/r6yoLgTghrLgkZlcmfuhqkX1D0Ko5whzOpOW8EhAdSEAN9S0UEWZ/HWsYMQ3tpQsnVp9xliW5Erz0iRUFwLwQU0LPbpc7NrFiJJFawLPL4wfXiymugqADwpaqDhHKiyVO3YyJH/RGsLSjm1swUqNF1FdCMABBS2UFCPoNcqC/OVqlC9GWrx7WUF1FQAHZLeQXKZKeV7R2kmfzIVGRET88MMPzfjg6tWrL1++TEBFyMiUKSipLs2XETE4IBPZLZSWJHLoaEDyQt+8eUPyBxvDwcMgPUlM3PiAHGQfF7p/obiNK7c9MV2UkZFx8ODB58+fq1SqTp06TZ061dvbe/bs2S9evMDecOrUKTc3t7Nnzz58+DApKYnNZnfu3Hn+/Pl2dnYIoVWrVjEYDBsbmxMnToSGhq5atQr7lKGh4b1793CvtjhHGne7bMj0VriPDMhE9looP0NC0Ik8Mpls9uzZDAZj3759Bw4cYDKZS5culUgkhw8f7tix47Bhw+Li4tzc3OLj43fs2OHl5bVz586NGzfy+fx169ZhI7BYrNTU1NTU1LCwMB8fn8ePHyOEvv/+eyL6ByHEM2PlvK8kYmRAJrJPSxML5AY8QhaamZnJ5/MnTpzo5uaGENq2bduLFy/kcvlHb/P09IyIiLC3t2cymQih6urqpUuXCgQCY2NjGo2Wl5d38uRJDoeDEJJKpUTUWYPNpcurVYpqFYOl3eekt3DktpAKVYkUXCNCLqCxt7c3NTXdsGHD0KFDu3Tp4uXl5evr++nbGAxGTk7Orl27kpKSxOJ/ogifzzc2NkYIOTg4YP1DDgMeQ1yh4Jnp7Pm1LQGpG3IKJdI3JOoCNDabfeTIkV69ep05c2bmzJmBgYHXr1//9G33799ftmyZu7v7kSNHnj179vPPP380CEHl1YnDZSgV1F9sAtRBagsxGEilVEnERF3F3a5duyVLlly7di0sLMzJyWn9+vVv37796D0XL1709vaeP3++i4sLjUarqKDy4ExZkYygzVpAGrJ3J3B5TLHw43yCi4yMjCtXriCEOBxO7969t2/fzmQyk5OTP3qbQCCwsvr3ROk7d+4QUUxjVEtVCCEWG4KQdiO7hWzb61dVEDIRgkAg2LRp008//ZSdnZ2ZmXn8+HG5XO7l5YUQatOmTVJS0rNnz/h8vouLS2xsbFxcnFwuP336NPbZ/Pz8Twdks9lWVlY1b8a9YLFQbt+B7ENkAHdkt5CFrd77V4RsO3l5ea1du/bGjRujR48eO3bsy5cvDx482L59e4TQmDFjaDTa/Pnz379/P2/evJ49ey5btqxHjx4FBQUbN250d3dftGhRVFTUp2N+/fXXz549W758eVVVFe4Ff0gQGZuzcB8WkIzsQ6uVFYrwnVlfb3Qgc6Ga6cLenJ4jLGwcyNsBCIhAehYyYtg5c0tyW/q5YTKJiqlHh/7RARTsDnLtYvT3nyUjZtd7vd28efPqPDlNoVCoVCrskOinLl26ZGJCyLQE8fHxS5YsqfMlhUJBp9NptLp3Cdy+fbu+av++XkL+uYKACNTMnXDh55weQy1s29f9HVxSUiKT1b2akkql9R26sbUl8BrYvLy8ZnyqvpJE5fJze3Jm/NBO7boA9ahpoYIMyetYoX9QC52FI+ZqqZU9x8kL1kK6gJoLv1u141i2Zj+IbIkXP8ffL1cqVdA/OoOyGXw6fWmskKue3iqjqgBKvH8pyngjhot2dQnFUzE+jy5TKpDfoBYxx2dKXEVWSmULmXey5aB4Ausu/qbVMuWtU4XUlkGCJ1H8zLfQPzqI+jm1sWm1H14s7hpg3qmXMdW14O/di4qYa6XefUy8+8BUwDpII1oIISSXqh7/WZKWKOr0hYlDRwOzVnpUV6QuIV+eniTKeCPmGDC+GGFhaAJnZOsmTWkhjFggT3gkSH8tlsuU7T0NGUwa14hhbKEnr9aCu9wxmDRRuVwslEvEyry0qmqp0qGjgXs3Ywtbrf86AJ+hWS1UQ8iX56dLROXVYqGcTqNVlON8onRcXJyPjw+Dgef1fwbGDJUScXkMQxOWlR3b3AY6p0XQ0BYiWr9+/a5cuWJk1HInJQZ40bVbygFAMmghANQCLQSAWqCFAFALtBAAaoEWAkAt0EIAqAVaCAC1QAsBoBZoIQDUAi0EgFqghQBQC7QQAGqBFgJALdBCAKgFWggAtUALAaAWaCEA1AItBIBaoIUAUAu0EABqgRYCQC3QQgCopYW2UOvWrakuAeiIFtpCubm5VJcAdEQLbSEA8AItBIBaoIUAUAu0EABqgRYCQC3QQgCoBVoIALVACwGgFmghANQCLQSAWqCFAFALtBAAaoEWAkAt0EIAqAVaCAC10FQqFdU1kCcgIEBPT49Go+Xm5lpbWzMYDIVCYW1t/euvv1JdGtBWTKoLIBWDwcjLy8P+XVhYiBDicrkrV66kui6gxVrWhpyPj49Sqaz9jKOjY9++famrCGi9ltVCwcHBNjY2NQ+5XO7kyZMprQhovZbVQu7u7t7e3jUPnZ2d/f39Ka0IaL2W1UIIoSlTprRq1QpWQQAvLa6F3NzcsBWRq6trv379qC4HaD0N3SMnKpeX5MmqpQoiBvfvNinvnXJk/9HvX1YQMT6TxTBrxTK2YBExONA0GndcSCxU3I0oKs6V2rsaSCsJaSGi6fOYWW/FppasbkPMre3ZVJcDiKVZLSQWKC4fzP1yrI2JpdZ/hVeJlLdO5gybYWNqrfU/C/gMzcpCJ3/MGPJ1Gx3oH4SQviF91Fz7yP05lRVauS4FjaRBa6G422VKJa1DNxOqC8FT9ltxcXZln3GWVBcCiKJBa6H89CojU11Y/9RmZMbK+VBFdRWAQBrUQgoZMjLVo7oKnBmaspCmrOYBITSohSpFcqVS1/7cVCpUUV5NdRWAQBrUQgBoI2ghANQCLQSAWqCFAFALtBAAaoEWAkAt0EIAqAVaCAC1QAsBoBZoIQDUAi0EgFqghQBQC7QQ2rhp9fUbl6muAmgraCGUkvKG6hKAFtPiFkpLS+3n7xsb+2jcVwGzZk/Enjxx8uikKYGDh/ScMm3MrrCtNdP/DhnWK/zsiZrPhu7YNOfbyQihfv6++QV5O3ZuHjHqn2mBo25enbdg+pBhveYtmH7+wpmaq3p/2LBq0+Y1hw7v7efvm5ycRPqPCzSUFrcQi8VCCJ04dXTCV1OWL1uHEDr+28FLlyPmzlly/tzNmV/Pu3f/r3PnT39+kKjrjxFCK1d8f/XyPYTQ7eio7aEbXZzdzpy6Mmvm/PMXzvz8y66axaWlp6alp27dHNamTTtSfkSgBTR0HrnGoNFoCCE/3+7jx01CCFWIKv4I/33ut0t79eqLEOrbZ0Ba2vtTp4+NGR2ENVtjXL9+qVMnnyWLVyOETE3NZkz7NnTnpsnBX5uamtFotIKCvIO/nORwOAT/ZECbaPFaCOPi3AH7R3Z2ZnV1dYcOHf99yaWDSCTKzc1u5FBKpTLp9Ss/3x41z/j4+CmVyoTEl9jDtvYO0D/gI1q8FsLosf+Z65DPL0EIcdj//onr63MRQlVVlY0cSiaTVVdXH/v1l2O//lL7+bIy/kfLAqCG1rdQDQMDQ4RQleTf6XIqK8UIITMzi0/frFDWMbcbh8PhcrmDBg7r3fs/t3uwtbEjpmSgC3SnhRwdXRgMxuvXrzq4eWDPJCcnGRkaWVpaIYT09Ni1V0fZ2Zn1DVIhqvDx9sUeVldX5+fnWllZk/ITAK2k9VmoBs+IN3DA0FOnf42JeSCsEN669efFS2fHjZtEp9MRQu7unvcfRItEIoTQyVPHSkqKsE+x2WxLS6u4uNiX8XFyufybmQseP753/cZlpVKZmBi/afOaZSu+lclkVP9wQHPpTgshhObPW/5Fzz6bt64dO27Q6T+OB0+cETxxOvbSgvkrzEzNR4zqO3Bwd6lU4t8/oOZTk4K/fvHy2ffrl1dJqjw9vQ8fPJ2Q8HL02IErVs0Ti0VbNoexIQKB+mnQhMBntmf1Gt3K1FqnZmOslqkidqV9u82R6kIAUXRqLQQA+aCFAFALtBAAaoEWAkAt0EIAqAVaCAC1QAsBoBZoIQDUAi0EgFqghQBQC7QQAGqBFgJALdBCAKhFg1rI1EpPB+8vr0TWbWC6BV2mQS3EMaAX50qorgJnJXkSRKO6CEAkDWqh9p6GpflSqqvAWVF2lbO3IdVVAAJpUAu17cA1MKI/u1lCdSG4efN3eQVfZtZOTHUhgEAadNUq5vHV0soKpbkt27K1Pk2DGrwJaAiV5EmEpdX8QsmoObbnzp1LT09ftWoV1XUBQmhcCyGE0pMq05JEUomSX9d2nUgk4nA4TGaj5h6qqqqi0+kkT35gZsNmsmj2rgZuvv9swkVERPTp08fQ0NDAwIDMSgAJNLGFPuOnn3764osv/Pz8Gvn+ESNGHD582MbGhuC6GqZUKnNycn777bf169dTXQvAk5a1UJNUVVWlp6e7u7tTXci/rly5IhQKJ0+eTHUhADda00J//vknk8kcPHgw1YWoS6FQMBiMsLCwpUuXYjPrA62mHYE9OjpaLBY3tX9WrVqVnp5OWFHNxGAwEEI9evQIDg6muhaAA61ZCzVVYWHh119//eeff1JdSANu3rz55ZdfcrlcqgsBzaTpa6GEhITdu3c344NmZmaRkZEEVIQzNze3gIAAoVBIdSGgmTR6LZSVlXX58uWFCxc247NKpZJGo2lL2CguLlYqlSwWy8zMjOpaQNNo9FrI3t6+ef0jlUp79+6tLf2DELK0tDQ2Ng4KCkpMTKS6FtA0GtpCEolkypQpzf74kydPRo4ciWtFhONwOLdu3cK26EpKdOcsJ52noRtymzdvXrt2LbbzqgVavHhxnz59xowZQ3UhoGEa2kJqSklJcXZ2xu4spKVOnjw5ZcoUgUBgbGxMdS3gczTuj2zOnDmZmXXfgq6Rnj59umfPHq3uH4QQth179+7dw4cPU10L+BzN+juLiIhYv35927Zt1RkkOzs7KCgIv6KoFBgYiBD68OGDXC6nuhZQN93ckNMxUqlUKBSGh4c3b/8kIJSmrIV27Nhx6dIl9ccRCAQPHz7EoyINwmazLS0teTwebNRpII1YC8XFxdHp9M6dO6s/1KFDh+h0+jfffINHXRqnoqLCyMjozJkzcH6d5qB+LaRSqTp37oxL/yCEeDyeDu8LNjIyQgi1atXqq6++oroW8A+K10JRUVGPHj3asmULhTVoI6FQyOPxnj171virDwFBqFwL5eXl6enp4dg/8fHxd+7cwWs0Tcbj8RBCFhYWvXr1EggEVJfTolHWQgqFwsjIqH///jiOefjwYUPDFjTjlIODw+3bt0tKSiorK6mupeWipoUKCwtHjhyJbdnjRaFQBAYGdu3aFccxNR+Hw3F0dGSxWH5+fsnJyVSX0xJR0EIKheLp06e4XwzHYDAGDRqE75jagsViPXv27N27dwghOAhLMgp2J+Tm5tra2uJ+JcKxY8d8fHzw2rOnvRYvXjxkyJCAgACqC2kpyF4LDRs2jMlkEnElz6+//urh4YH7sFpnz549CQkJCCGZTEZ1LS0CqWuhp0+furu7E5H4q6qqysvLNWG+OM1x/fp1sVg8duxYsZjAGYn19PRInulS05DXQllZWebm5jCdJ5m2b98+atQoCwsL4hbBZrPx3S2kdUjakPvuu++Sk5OJ65/JkyeXlpYSNLj2CgkJsbW1ValUsNebOI2amVpNmZmZixYtsra2Jmj8N2/e0Ol0c3NzgsbXaoaGhjKZTKVSicVi2AQgAuEbcrm5uQih1q1bE7oUUB+lUsnn82tmUZVKpfhGF9iQI3ZD7uzZs2fOnCG6fwQCgUKhIHQROqBmIoqysjKqa9EpBK6FBAKBSCQiun9yc3Pnz5+Py7VGOqlmLVQDWx3J5fJG3mDm82AtRNRaqKysLC0tjYTtt8TERB2+uoEI2OqIRqPx+XylUkl1OVqPkLVQSkrKpk2bTp8+jfvIoKk+XQtt2bKFTqdbW1ufO3duzZo1ffr0uXz58tOnT9++faunp+fp6Tl9+nRbW1vss/v374+JidHT0+vbt6+Hh8f69evPnDlTe8pVWAvhvxZSKBQ8Ho+c/lEoFM+fPydhQbqEyWRmZGSkp6dv2LDBy8srKSnpwIEDbm5u69evX7FiRXl5eWhoKPbOyMjI69evz507d9++ffr6+r/99htCSNunRsIdzju1lUrlzZs3hw4diu+w9bl58+aTJ0+6dOlCzuJ0A41GKyws3Lt3L4fDwa6EPXDggLm5OXYNklwu/+GHH7BL+m7fvt2rV6/evXsjhIKCguLi4qiuXRPh3EJ9+/aNiorCd8zPEAqFcAl0M7Rp0wbrHywaFRYWHjly5O3btzVHYMvLyw0MDDIzM2uf/N6rV6+kpCSKStZceLZQWVnZvXv3yFzR68x8cSSrfWjo77//3rhx44QJE2bOnOng4PD48eOtW7cihMRisUqlqn3jI5hXtU54/rmbmpqSvKF848YNOB9ZTTdu3PDw8JgxY4ajoyOdTq/ZvaSvr48Qqq6urnknHFCqE55/8YsWLYqJicFxwAaFhYXB2V9qqqioqH0eas0sfCwWy9LSsvbkzH///TcVBWo67d67MmTIED09Paqr0G7t27d/8eLFq1ev5HJ5ZGQkttopLCxECHXv3v327dvPnz9XqVSRkZEikYjqYjURnllo7969OI7WGMuWLSN5ibpn2rRplZWVGzZskEgko0aNWrRoUWlp6ffffx8SEjJp0qT8/PzvvvvO1ta2U6dOgYGBu3fvZrFYVJesWTRiNtNmu3Hjhr+/P6yIPuPTQ6uNJ5FIiouL27Rpgz08d+5ceHj4hQsXar8HDq1CFgL/UV1dXfOtev78+QULFly6dEkgENy/fz8yMnL48OFUF6hxyLheiDiQhXAnEomMjIywM1AnT54sEAhu3759/PhxCwuLkSNHTpgwgeoCNY52b8iBBjV1Q04kEunr6zf+Fp2wIafde+TguBDuDA0NW+wtbpsHshD4j9pZCDSGdq+FIAvhTiQSwSXATQJZSPc1aYrgX375JTAwELteqDHodHoLv/xBu1sIjgsBykEWAv/x6tUrQmc/1T3avQqGLIS70NDQnJwcqqvQJtq9IQdwFxoaGhwcbGdnR3UhWkO7WwiyEKAcZCHwH5CFmgqyEPgPyEJNpd0bcgB3kIWaSrtbCLIQoBxkIfAfkIWaCrIQ+A/IQk2l3RtyAHeQhZpKu1sIshCgHGQh8B+QhZoKshD4D8hCTaXdG3IAd5CFmkq7WwiyEF4GDBjAYrFoNBpCCPuToNFoJiYmZ86cobo0TQdZCCCEkJ6eXnFxcVFRUVFRUXFxcXFxcWlpaf/+/amuSwtAFgIIIeTn5/fR9oiDg8O4ceOoq0hraPeGHMBLenr6ggULsNnosZtJzpo1a9asWVTXpQW0ey0E88jhxcHBwdfXt+Zhu3btYBXUSJCFwD+mTp1qbW2NrYIGDhxoYmJCdUXaQbvXQsOHD699z0OgDkdHRz8/P4RQ69atx4wZQ3U5WgOyEGVkEqWoXIGQBv3+s7Ozf/jhhyFDhowfP57qWv6DqUfnmWnoLRS0u4WuXbs2cOBArVsRvX8pSngkKMmTmrdiS6tg6tCGcXnMomxJBz9e7zEWjXg7qfBsoUWLFgUFBfXs2ROvARs0cODAc+fOaddWe9JjYdprcbehVlwjmP29CWQSZe77yrfPysYtakPXpN8cZCFSJcUIMlMq+02wgf5pKj0O3cHT0Kuvxfl9mnUKn3ZvyGmXapnq2pG8AZNbU12Idkt4wDezZnXoqik3NcJzLaRUKkluyGvXrkmlUjKXqI6SXGl1NXxhqUvfkJmfXkV1Ff/Cs4WWLFny999/4zhgg/bs2VNVpUG/zc8TlFZbt9WnugqtZ2qlVy3ToG8iPFuIwWCQfJ8M7cpCSrlKIob9b+pSKpGorJrqKv6F57723bt34zhaYyxevJjkJQLwEchCAKgFshAAaoEsBIBaIAsBoBbIQgCoBbIQAGqBLASAWiALAaAWyEIAqAWyEABqgSzUom3YGLJi5bxmf/xCZLj/wK54FqSFIAsBoBbIQgCoBbKQRhs12v/ChT8WL/2mn7+vsEKIEIq8eHZVyIIRI/uOHT940+Y1uXn/XAWtVCp3//S/seMHTwwecfTY/tjYR/38ffn80sYvq7KycsuP68Z9FTB4SM85306+dPlczUtZWRnLln87fGSfUaP9Fy/95mV83KcfVygUK1bOmzx1tEAoCD97YsiwXjUvFRYW9PP3ffz4PkIo4typwDEDHj26N2bcoP4D/CZPHX3r1p/q/ZIoBllIo7FYrGvXLzo5ue4I3c/V5yYmxu/7eYeHh9emTTtXh2wsK+Nv/XEd9s5z509fvRa5cMHKgwdP6etzj/36C0KoSf87Vq9dlJeXs3nTrojw6717++/Zuz357WuEUFkZf8HCGVZWrQ4fOrN/33FTE7PNW9Z+OgNm6M5N794lh27/2Zhn/JmlMBhMsVgUfSfq9MnLly5G+/cfvC10Q3Z2ZrN+PRoBz7/43bt3d+/eHccBG7R48WJ9fV2+DpRGo/F4xgvnr/Dt0o3JZLq7ex4/FjEpeIaPt6+fb/evxk9OTk4SCAUIoZu3rvX+sn/fPgOMecaTgmdwDQyatKDYJ48TE+NXLv++g5uHsbHJpOAZnp7ev584jDWnHpu9Yvk6W5vWdnb2K1esr6qqvHzlXO2Pnzh59O7dWz9u/cnWpuGZIeRy+ZjRQfr6+jwj3vRpcwy4BtF3bjb9d6Mp8NydoFQqaTQado8ackRFRfXv31+3b+7g6uJe828Gg5GXl7P/l13Jb5NqbudYXsY3NDDMyEgbEjCy5p29v/RPSHjZ+KWkp6dyOBwHB8eaZ1ycO0TfiUIIpaWnOju7MZn//KkYGBi0sWv77l0y1uE0Gu12dNTx3w7+sH5bx45ejVyci0sH7B80Gs3W1i4rK73xpWoa7c5Cu3bt0vk5tWt/QTx+fP+775e5urr/FHbkzu1nodt/xp4XiUUqlYrL/XfNY2zctLn1SktLOJz/rM+5XG5VVSVCiF9awmFzar/E0devrKrEbualUCi2bf8BIfTRez6v9uY3m8MRi0VNqlajQBbSJteuX/T09J41c76TkwuNRhOJKrDnufpchFB19b8zCpSVNWFHArZukUj+s2NGXCm2MLdECHENDCRSSe2Xqiorzc3+nVV0+bLvBg8evi10Q1kZv87BFcqPZ4yofUdkqUTyUfdqF8hC2kQoFFhaWNU8fPjwDvYPFotlZWWdkfGh5qXHMfebNLKri7tEInmfmlLzTHJyUjsHR+yl5OSkmv4UVggzs9JrNvnodPqQgJGLF4Zw9bk1+zZYLD2pVCqXy7GHWZkfb6e9jH+G/UMqlWZlZ9TegNQ6cFxImzg5ujyLi30ZHyeXy8+dP409WVCYjxDq2aP3rb/+fBYXq1Kpzp0/XVEhbNLIXbv2tLW1Cwvb+jblDZ9feuzXX5KTkyaMn4IQGjFirFgs2hW2tbCwICMj7X/b1nPYnKFDAmt/XF9ff8OG0PhXzyPOnUIIubt7qlSqqJtXsT3aZ8J/q/1mOp0eGRmelZWhUCh+PX5AKpX69w/A49dDDe3OQjp/XOgjX389r1vXnuu+XzYooEdhYcHqkI1uru6r1yy6HR01bepsT0+fVSELpkwdnZmZPm5sMEKIyWQ1cmQmk7ll0y4ez3je/GnBk0c+f/F086adnp7eCCG71m1+WL8tPT01KHj4kmWzEUJ7fjpq8MkePxdnt6lTvjly9Oe0tNQObh5zv11y+PDefv6+m7asmTljXs1dkLFdCF+Nn7xsxbcDBnW7eu3C6lUb2rRpi/evijx4Tgi8dOnSCRMmkLktt2fPntmzZ2vLttybWGF2qqTnCKtGvLfJJBJJUVGBvX077GH42ROnT/969co9IpaljguR4b8cCIv+62mzRyjKksTfLRm7yA7XupoPzpHTEeFnT4Sf/f2bWQsH+Ac8f/E04typkSPhTo9k0O7jQlp6fyEiTJ82WyAou3Xr2pGj+ywtrUcHTpgUPAMhNGJk3/o+EhKyodcX9b4KGgnuL0QeQjfk6pNfkFffS6YmZhxOEw7maAhd3pCD40IayKaVLdUl6DjIQgCoBY4LAaAWOC4EgFrgHDkA1AJZCAC1QBYCQC2QhQBQC2QhANQCWQgAtUAWIg9Tj6ZvwKC6Cq1Ho9N45ho0WwZkIfKYWunlpur4TA8kKMmV6OmTdypzgyALkcfSjs3Wp38yjwBomsoKuZ0zl+oq/oXnmdqgQZnJlbE3+ENnaspZxlon4UFZBV8aMK0V1YX8C7IQqdp24PYda3nx58y8D5VigZzqcrSGQq4qzJQDTk+oAAAgAElEQVTE3+VLq6o1qn/geiFqlBdXP79dlv2+UiVHYhE0UsMs7dhMFs21C69jTx7VtXwMrheigIkly3+iFUJIpUK4XOMrFovHjRt348YNHMZqtKioqEePHm3ZsoXMhWogyEKg+d69e2dnZ8flalC4Jx9kIa0XFhYmk8koWbSLi4s2bgXgC44LabdZs2ZNmzaNwon5161b99dff1G1dE0Ax4W029GjR83NzSksYO3atffuadxsdWSCLKStdu3aNWHCBDs7OMREMchCWiksLGz48OGa0z8HDhxQKFroaReQhbTSsmXLXF1dqa7iX9bW1tu3b6e6CmpAFtIyhw4diomJobqKj40ZM2bChAktc4sAspA2uXLlSqtWrbp27Up1IXVQKBQymUxbbhGAI8hC2mTkyJGa2T/YNsh333334MEDqgshG2Qh7RAREXHixAmqq2jAd9999/jxY6qrIBucI6cF4uPjLSws+vfvT3UhDTA3N1+zZg3VVZANshDA2Y4dO1auXEl1FeSBLKTR7t+/HxISQnUVTdO6deuwsDCqqyAPZCHNlZ+fL5VKte54S3Bw8NixY2tu963z4LiQhlIoFEZGRoMGDaK6kOawsbHR4a+2j0AW0kQZGRkrVqw4f/481YU03zfffDNv3jwfHx+qCyEcZCGNI5VKU1NTtbp/EEIbN27UwLMoiABzJ2gWlUqVkZHh4OBAdSGgsbQ7CwUGBupYFuratavO9I9UKt25cyfVVRAOspAGiYuL69y5M8lfQ4Q6cuSIUqmcM2cO1YUQCM8WUiqVNBqNhsucNI1z6dKlIUOG6MaK6Pnz5506dWKxWFQXgrPCwkJLS0td+l74iHYfF9q/f79u7DwdO3ashYWF7vUPQojH4wmFQqqrIJB2nyOnG1koNzf3zJkzWvqDKBSKzx9FpdPpv/zyC47XqOvp6ZG5pdMgyEIUi42NdXZ2pnYKEXWIxeIGNwQUCoVUKsVrujljY2ONWl3judKQy+VKpRLHARt06dIlrT4utGLFColEor3900gMBkOHp2vEs4WWLVsWGxuL44AN0uosVFlZ+eOPP/bt25fqQsigVCpFIhHVVRACzxZisViQhRrpyZMnWVlZFE6hSDI6nU6j0SordfAOZZCFKHDgwAEulztt2jSqC8FBY7JQDaVS+fkv2S1btohEom3btn3mPZqWhfDcIyeXy+l0OpkrIi09LjR37lyqS6AGjUZrsIu0DmQhUsXHx9+5c4fqKihDo9HKy8t1bNJGyELkuXPnTnR0tOZPgdBs2dnZAQEBiYmJ2MO7d+8GBARcuXKl9qsFBQUymezWrVtLliwJDAxcsmTJxYsXa6cJGo324sWLtWvXBgYGLlu2LDU1laKfprHw/IvftWtX9+7dcRywQfPnz9eiicv69++/fPlyqqsgUJs2bSwtLd+8eYM9fP36tZWVVXJycs1DAwMDNze32NjYsLAwJyen48ePT58+/eLFiwcPHqwZJCsr6+rVqxMmTNi4caNSqdywYYOGx3U4LkSGtLS0Y8eOUV0FGby9vVNSUrB/JyYmDhw4sGal9Pr1a+wk2qioqA4dOixYsMDU1NTb23vKlClXr14tKyvD3lZeXr5gwQIvLy8vL6/g4OCSkpKaETQTZCHCpaen//777zNnzqS6EDJ4eXklJSUhhAQCQWZm5rBhw/h8flFREUIoKSnJx8dHqVS+efPG29u75n+ct7e3UqnEPoUQcnBwqDnW7OHhgc0hQd0P1DA898jZ2dmRvLdx/Pjxmp+FHBwcNm7cSHUVJPHx8REKhdnZ2enp6Y6OjmZmZh06dEhMTPT19c3Pz/f19ZXJZNXV1X/88ccff/xR+4Pl5eXYPwwMDGqexLbSNfwsVTxbaNWqVTiO1hizZ88meYlNVVFRcevWrbFjx1JdCEnMzc3btWv35s2btLS0jh07IoQ6duyYnJzMYDBsbGysrKywxhgwYECvXr1qf9DGxgb7h0QiqXlSLBZj53qT/nM0AWQhYgkEglOnTlFdBamwbbmkpCRPT09sYywpKSk+Pr5z587YG9q3by8SiVq3bo0FHnd3dzMzM0tLS+zVrKysmi569+4dNjEddT9NwyALEcvIyGjMmDFUV0Eqb2/vhISEtLQ0LMl4eHhkZWW9fPnS29sbe8OMGTNiYmLu3buHRaD//e9/ISEhNXdc5nA4P/30U0VFRXl5eXh4uKWlJTaOxsJzQw6OC33K2Nh4ypQpVFdBKm9v78LCwjZt2piammLZpm3btunp6TUt1LFjx/379589ezYoKEgikXTo0GHDhg3Y/8fq6moPDw97e/tJkyYplUpXV9cffvhBo64O+hScI0esioqKqKio8ePHU10IUZp0jhwuNO0cOchCxBIIBGfOnKG6Co2jUqlKSkqorgIfkIWIZWRkNG7cOKqrAASCc+SIZWxsPGnSJKqr0Dg0Gs3CwoLqKvABWYhYkIVwB1kIT5CFtBRkobpBFvoUZCGdB8eFiKXzWYjD4TCZzfkravZpO81bHHEgCxFL57MQgCxELMhCdZLJZGTeRIdQkIWIBVmoTjQajcFgUF0FPuC4ELF0Pgs1D4vFevjwIdVV4AOyELEgC9VHoVDoxooIshCxIAvVSSaTffnll1RXgQ/IQsSCLFQnyEJ1gyz0KchCdYIsBBoLslB9IAvVAbLQpyAL1QmyUN0gC30KslCdIAvVDbLQpyAL1QmyEGgsyEL1gSxUB8hCn4IsVCfIQnWDLPQpyEJ1gixUN8hCn4IsVCfIQqCxIAvVtmTJkuzsbBaLpVKp5HI51ksKheLcuXNUl9Z8kIWIBVmott69excUFKSmpn748CEzMzMzMzM1NVXz72P3eZCFiAVZqLbAwMCP5pjHZv2lriIcQBYiFmSh2uh0+ke3hOJwOBMnTqS0KHVBFiIWZKGPKJXKiRMnfvjwAXvo6Oh49uxZqotSC2QhYkEW+gidTh83bpyenh5CiM1m68BtLyALEQuy0KfGjh3btm1b7M6iw4cPp7ocdUEWIhaFWUilQCqlJv5HQ/Qxo8dy2PrBEydRXky9/yka+3uGLEQs8rOQUoEeXirOfFvJNWIWZmr0KlqTtWqnX1khb9fB4MvRDUyfj2cLyeVyOp1O5oro0qVLQ4YM0eQVUU5OzsKFCy9evEjO4iSVymPr0/oH2Rqbs4zMNGjudm0kLK0WlFTfOZs3Z6sji1PvnfYgCxGLzCwkkyh/25Qx9XsnO2cu9I/6eOasNq7cqd87Hf7ug6r+3WSQhYhFZhZ6eLFk4CRbcpbVctBoaMCk1vcji+t7A55/8bt27erevTuOAzZo/vz5+vr6ZC6xqSoqKkg7AezdywpzG43+QtFSZq30Ul+J6nsVjgsRi7TjQuXF1W1cDRgsjb45tpbiGDCs2nDEAnmdr0IWIhZpWUilQmWFGv1totVK86T17XeDLEQsOEdO50EWIhaZWQhQArIQseAcOZ0HWYhYcI6czoMsRCzIQjoPshCxIAvpvHpvnlxSUkJOBTQazdzcvHmf1fxz5LAsBJfc6TA810ICgUAmkzX1U+qc5wpZCFAOzxai0cg+NA5ZCFAOzxbi8XjYBb2kgSwEKIdnC5F/9R4cFwKUw7OFhELh57PQgwcPAgICysvL8VoiZCGqpKWl9vP3TUh4SXUhDcvJyern7/ssjqgjlpCFiAVZSOdBFiIWZCGdV+9xoU+9efPm9OnTKSkpxsbG3bp1mzx5MpfLRQht3bqVRqP1799/165dVVVVbm5us2bNcnNzwz519OjR6OhofX39vn372tnZ4Vt9ZGTksGHDNHlFpLHHhSIvnn306G7YroPYw2kzxpWXl12+GI093LxlrbhSvO3HPXK5/Nivv8Q+eVRUVNCxo/foUV91796rZhCpTPrLgd33H9xWqVT9+w3+ZtaCz9/yRKVSXYj84+bNa9k5mW3tHXx9u389Yy72kdevE34/cfjt29fGJqY9un85bepsAwODmlJjYx8mJyfpsdlenTrPnDm/ta0dQuhCZPiZP44vXbLmhw2rAgO/Wjh/hbBCeOjQnus3Lhsbm/h26fbNrIXW1q1qlr4rbOu1Py+am1v0/rL/ooWr8PpNNnYtlJubu3btWolEsnv37vXr16enp69cuRKbnJ/JZCYnJ0dHR2/dujUiIoLNZu/cuRP71LVr165duzZv3rw9e/a0atXq9OnTeNWNOXDgAGSh5nFwcEx+m6RQKBBCZWX8wsJ8LDZgryYmxft26YYQ2rsv9PyFM6MDJ5w5fbVPb/8fNq66/yC6ZpC9+0JdXDqsDtk4KfjrsxEnr9+4/PmFRkaGnzr967ixweFnro0YMfbP65fCz55ACOXkZq9YNU8ilfy87/jmjTvT0t4vXTYb++tKTIzf9/MODw+vTZt2rg7ZWFbG3/rjOmw0PT29ykrxlSvn16zeNHrUV3K5fPWaRSWlxWG7Di5csLKouHD12kXYIAih478d7NSpc9iug1+Nn3zxUsSdu7fw+k02di109+5dJpO5fv16Y2Nj7C4X06ZNi4mJ6d27N0Koqqpq6dKlcrmcyWT27dt3165dlZWVXC738uXLX375JXY/s0GDBqWkpOTm5uJVOkJozJgxmrwK0uQs1N7BSSKRpKWnOju5xr963r69s6GB4auEF3Z29gUF+cXFRV06d5NKpTdvXQueOH3kiLEIoaFDRiUlvTpx8kif3v7YIF06dx3gH4AQ8vH2vXnr2t27t0YMH/OZhb5KeOHq6j548HCE0PBho318/KoqKxFCt2/fYDFZmzfuNDY2QQitWP79xEkjHj2+17fPAHd3z+PHIuzs7JlMJkJIXl29dt1SgVBgzDOm0WgSiSQoaFpnHz+E0KPH95KTk34/ft7evh1CqE2bthHnTvH5pdiifbx9Bw4Ygv0j8mJ4YuLL/v0G4fKbbOxa6M2bN66urlj/IISsra1tbGySkpKwh23atOFyuVgWMjQ0RAiJRCKVSpWXl2dvb18ziLOzMy5F15g7dy5koeYxNjaxtbVLTIzH1jkdPbw6dOj4+nUCQigh4YW5uYWDg+O7d8kymczPt0fNp7y9uqSlpQqEAuxh7ZfcO3jm5ed8fqEdO3o9f/4kdMemqJtXBUJBa1s7JycXhNDr16/c3Dyw/kEItWplY2trl5D4EiHEYDDy8nLWrF08fGSffv6+a9ctRQiVl/FrxnRz9cD+8eHDey6Xi/UPQsjF2W3d2i1WVtbYQ8+O3v/+7DwTHI+FNHYtJBKJ3r17FxAQUPvJsrIy7B/YCdoqlar2TrnKykqFQlH7T5zD4eBU9j+0IgtdvHhRA7MQQqizj9/r16/GjJ7w6tXzGdO/ZbM5e/ZuRwglJL708fFDCIlEFQihhYtnfvTBsv//ajcwMKx5ksvlCgQNHK4YNzaYyzV4HHN/e+hGJpPZt+/AOd8ssrCwFIkq3qa86efv++lSHj++v2798knBM+bMXuzo6Bz3/MmqkAW131azB0ssFrHZ9f6BMZhNiP1N0thxzczMPDw8pk6dWvtJHo9X+6FQKKzdMFwul8Fg1G533HPLgQMH+vfvr8ktZGRkNGzYMKqrqFuXLt0OHdojEJSnpaV29umKfd8LBOWJSfHBQdMRQuYWlgih5cu+a926Te0PWlm1KijIQwhJJP/+DxVXimtWI/Wh0+nDh40ePmx0RkbaixdPfztxWCwW/bhlt5m5haen94zp39Z+szHPBCF07fpFT0/vWTPnY09iXV0nLtegqqpSqVSSfMVNY1vIwcEhOjra09Ozpr7MzMyPbrf00XEhGo1mZWWVnJxc88zTp0/xqPlfkIXU4ePtW1CYH33npqOjM7Zz1dXV/fbtG1lZGb6+3RFCdq3tsV+vj/c/64eyMr5KpcLejBB69/5tzQ66lJQ3rW3b1L80hBC6efOai0sHBwfHdu3at2vXvkJU8ef1iwghx/bOt/7606tT55q/royMNDs7e4SQUChoZW1TM8LDh3fqG9zN1V0ikaS8S+7g5oEQysrKCPvpx4XzVxL9F9LYfh0zZoxSqTx48KBEIsnJyTl27Ni3336bkZFR+z2fHhfq3bv3o0ePHjx4gBCKiIh4+/YtrsVDFlKLsbGJi7PbhQtnOnp4Yc909PCKvBjevr2TubkFth0xfdqcEyePJCbGy2Sy+w+iV6ya99OebTUj3Ll788nTGITQX7dvJCcn9WsooEffiVq/YWVMzAOBUBAb++jhozvYoseNm6RUKn/+ZZdEIsnOzjx0eO/XsyakpacihJwcXZ7Fxb6Mj5PL5efO/7NHt6Aw/9PBfX27t27d5vDhvQ8f3X0WF/vTnm3FRYVt2zrg+jurQ2NbyMjI6ODBgxwOZ+HChbNmzUpISFiyZImTk1Pt93x6jtzEiRMDAgIOHDgQEBDw5MmT2bNn43sqXWRkJJwjpw4fH7+8/FxPTx/soYdHp7z8XB9vv5o3BE2YunLF+jPhv40Y1XfP3u22NnbLl69DCFXLqxFCs2bOP3xkbz9/3yNH9wVNmDokYOTnF7d82bp2bdt/9/2ywNH+O3Zt/qJnn2VLv0MI8Yx4x46e1efoz5k7eer0sfGvnq9c8b2LsxtC6Ouv53Xr2nPd98sGBfQoLCxYHbLRzdV99ZpFt6OjPhqcyWTuDP1FqVKu/2HlqpAFHH39//24h0lYBKpR77T0zbjkTiAQ6OvrN+MEBQuLBibPr8/AgQPPnTtnYtLAJjiFBALBtWvXSNiWKyuqvnY0L3B+W6IX1DKd350xfomdoUkdDand58hBFgKUw3M199EOOhLMnTuX5CU2VUu71+qZP37744/f6nypbbv2P+/9lfSKCIdnC310XIgEWnFcSDPPkSPIiBFj69upwGQQHksogedPhR0XIvNkba04LqSZ58gRxMjQyMjQiOoqSAVZiFiQhXSedl8vBMeFAOXq3ZBrxo5mqVTKZDI/f8UIviALAcrhuRZauXLlkydPcBywQXC9EKAcni3EZrPJXAVBFgKaAM8W2rFjR7du3XAcsEGan4WEQmF4eDjVVQAC4dlCUqkUu5CYNJp/jpxQKDx79izVVQACQRYiFo/HmzBhAtVVAAJBFiIWj8cLCgqiugpAIMhCxCItC6lUyMRao79NtJqZDVuF6j5zALIQsUjLQmbWrMw3ItJnNW8RqqXKgowqI5O6t7AgCxGLzCzk7GUoKGry/Z1Ag8qLqx09Dep7FbIQscjMQl0DzO9E1HFFNFDT3bN5PYbVeyPGeq9aBbgQCoXXr18nrYv4BbJrR/P7B9kYW5J6sqJOUqlQeZEs+kze2AWteRas+t6GZwvBOXKfysnJWbhw4cWLF0lbYlmh7EkUP/WVqJ27YXmR5gZFhVLJIHe2qiYxseZkvKlw9jbqMcycZ/a5a4LwbKFFixYFBQX17NkTrwEbpPlzJ5C8FqqtrFBzc1F1dfXUqVP/+OMPqgv5DJqpdb1rntrwvOQOstCnKDwuZGqtudtyMhkSSvI1ucLGgyxELArXQppMJpP17ds3JiaG6kJwAMeFiAXnyOk8OC5ELDhHTufBcSFiwTlyOg+yELEgC9UJslDdIAt9CrKQzoMsRCzIQjoPshCxIAvpPMhCxIIsVCfIQnWDLPQpyEI6D7IQsSAL6TzIQsSCLKTzIAsRC7JQnSAL1Q2y0KcgC+k8yELEgiyk8yALEQuykM6DLEQsyEJ1gixUN8hCn4IspPMgCxELspDOgyxELMhCOg+yELEgC9UJslDdIAt9CrKQzsOzhXbu3BkXF4fjgA2KiooSi8VkLrGpIAvVKS4uztXVleoq8IHnPHLl5eU4jtYYe/fuffPmjb6+vsbeIgWy0EdKS0u3bdtWWVm5d+9eqmvBiUr7lZWVbd++neoq6hUeHl5UVER1FRrh0KFDgwYNunPnDtWF4Em7sxDGxMSkXbt2169fJ3/RjfH27dvY2Fiqq6DYnTt3Bg4ciBC6efNmv379qC4HT9o9p3ZtfD7fzMwsPj7e29ubkgLqk5KSIhaLO3fuTHUh1MjKytq2bZuRkdHq1atNTU2pLgd/2j2ndm1mZmYIoSNHjkyePLlHjx5UlfEpncnNzbB79+6HDx+uXr26a9euVNdCGKq3JPF3+/Ztqkv4j8rKyt27d1NdBdmuXLnSs2fP06dPU10I4XQhC33E398fu5NxSUkJ1bUghJC+vn5UVFRpaSnVhZDk7du306dPf/HixZ07d4KDg6kuh3g4tuPChQsfP36M44DqEAgEq1evprqKf8TGxhYXF1NdBeHkcvnmzZsnTZqUlJREdS3k0e5z5D6Dx+P973//QwhduXKF6lpQt27dLCwsqK6CWOHh4V988UXHjh1PnTrl4eFBdTnkwbOFduzY0a1bNxwHxIWVldW8efOorSE+Pv7cuXPU1kCc58+fjxs3LicnJzY2NjAwkOpyyIbnHjny77XaGN27d8d21uXn59vY2FBSg6Gh4YULF8aPH0/J0okjEAi2bdvG5/N37tzZrl07qsuhhnZfL9RILi4u2HlZv//+OyUFODk5zZ07l5JFE+fXX38dM2ZM//79Dx061GL7R+uvF2qSESNGCIVCqk5L7dOnDyXLJcKDBw+GDh0qlUqjo6Oxcw5ashZ3vVB1dfXjx4+dnJzs7OzIXO6pU6dcXFy0/Qhjfn7+tm3bGAzG6tWrraysqC5HI+jgcaHPY7FYX3zxxYIFC8rKykhetLZfYbZv377Zs2dPmDAhLCwM+qdGi8hCH2GxWJcuXRIKhYWFhaQtdOTIkcOGDSNtcfi6ceNGnz59eDze1atXqToHUmPpzjlyTdW2bVuBQDB58uQTJ07Q6Xh+ldSJx+PxeDyEUGBgYFlZ2f3794leIi5SU1O3bdtmY2Nz/fp1AwMDqsvRRC0uC30kJSUlJSVlxIgRNBoNe8bf39/NzW3//v14LWL06NFisbisrEyhUGC9ymQyQ0JCRo8ejdciCLJt27b4+Pg1a9Z4eXlRXYvmanFZ6COurq4jR46Uy+UnTpzANrcEAsGHDx9evnyJ1yIYDAafz1epVDXrOisrqy5duuA1vvo+PR564cIFPz8/Jyen8PBw6J/Pa4lZ6FMsFksgEFy9ejUnJwchVFxcHB4ejtfgkydPNjY2rnmoUCh4PJ69vT1e46tp8+bNOTk53bt3xx6+evVq4sSJ7969e/bs2bhx46iuTgu03Cz0kYULF3bt2hVbUdBotKSkpNevX+NyrldgYODLly+vX7+ObTPT6fQvvvgCj5JxEBMT8+DBA4SQXC4fOnSor69vbm7upk2bnJ2dqS5Na7T0LFSjV69eEomk5qFKpRo+fPjGjRvxGj8oKOj9+/c0Gs3MzOx///ufhmzIjR8/Pi0tDcuBKpVq8+bNQ4cOpbooLYPnhlxVVZVcLsdxQNL06dNHLBYrlcqaZ2g0WkJCQnp6Ol6L2LRpU6tWrbDLhzQkXezevTsnJ6dmPwqNRtuzZw/VRWkfPFsoJCTk6dOnOA5Imvv37/fv39/FxcXKyorNZmPXgWRlZeE4i6KLi8vUqVM5HI67uzuTief2c/MkJCTcvHmzurq69pPFxcXUVaSt8NyQW7169ZgxY7T3HJaUuIp3r8rEYkl5oUJeXS1XyBFC5ubmOC6ivFzAZrP19Tk4jtlIPDMWz5zVqZexuY0eNh15WloanU5nMpn6+vosFovD4bBYLIVCoQlXWGkRyEL/+PNYvpG5nomFnkVrjk7+RqqlqpLcqtR4Yed+pi6dDbds2cJms62srMzMzDgcDpfLNTQ01JAtTO2CZwtVVVWxWCxN2EppqqjfC0ysOB49TaguhAz3zxU4dOR6dOdRXYiOgCyE3sQKDYz1Wkj/IIT6jG/17oWogq+VO340EJ4txOVytXEVlPpKZGlHQTihkIExK/OtRk/nr0Xw/Ivftm0bjqORRqlEFq1bVgtZ23MqymRUV6Ej4LgQKsqR4Ppr0AJKpUos0LKzGTUWZCEA1AJZCAC1QBYCQC2QhQBQC2QhANQCWQgAtUAWAkAtkIUAUAtkIQDUAlkIALVAFgJALZCFAFALZCEA1AJZCAC1QBYCQC2QhZoj8uLZVSELRozsO3b84E2b1+Tm5WDPX7wUMWbcoKysjBkzv+rn7zvzm6Com1exlypEFXt/3jFp8qihw79cumzOn9cvIYQ2b1m7bPm3NcNOmzFu1Gj/moebt6xdvXYxNtXoocN7Z8z8atiI3iFrFsXGPsLekJaW2s/fNzb20bivAmbNnkju7wD8A7JQkyUmxu/7eYeHh9emTTtXh2wsK+Nv/XEd9hKLxRKJKvbuC125/Ps7t5/16T0gdMemwsIChFBo6MY3rxOWLFnz26/nO3TouPun/71+ndC5c9fkt0nYXP5lZfzCwnyEUE5O1j8LSor37dINIbR3X+j5C2dGB044c/pqn97+P2xcdf9BNLY4hNCJU0cnfDVl+bJ1lP5WWi48N+RaSBZyd/c8fizCzs4e+2Hl1dVr1y0VCAXGPGPsRpTTps52d/dECA0eNPz4bwdTU1OsrVu9SngRNGGqn293hNDsbxb26TPAmGdiYWElkUjS0lOdnVzjXz1v397Z0MDwVcILOzv7goL84uKiLp27SaXSm7euBU+cPnLEWITQ0CGjkpJenTh5pE9vf2wiUj/f7uPHTaL6t9JyQRZqMgaDkZeXs/+XXclvk2puflxexsdaCCHk5vbPZPZGRjyEkEhUgRDy9PSOOHdKICj36tTZz6+Hq0sH7D22tnaJifHOTq6JSfEdPbz09fVfv04YNjQwIeGFubmFg4NjYmK8TCbz8+1RU4C3V5cbUVcEQgH20MW5A7m/APAfeLaQ9s4j1ySPH99ft375pOAZc2YvdnR0jnv+ZFXIgtpvqJmluraQVRuuXDl/5+7NiHOnDA0MR4+eMHXKN0wms7OP3+vXr8aMnvDq1fMZ079lszl79m5HCCUkvvTx8avpwLZ5ffkAAA3zSURBVIWLZ340YBm/FPtV67HZBP/E4HPw/HMPCQkJCgrS+XtxXrt+0dPTe9bM+dhD7E+8QTwj3uRJX08KnpGU9Orho7snTx0zNDT6avzkLl26HTq0RyAoT0tL7ezTFVvFCQTliUnxwUHTEULmFpYIoeXLvmvduk3tAa2sWvH5JcT8iKAJIAs1mVAoaGVtU/Pw4cM7DX5EIBRER0cNHTKKw+F4enp7enqnpqa8e/8WIeTj7VtQmB9956ajozOXy0UIubq63759Iysrw9e3O0LIrrU9m83G3omNVlbGV6lUXC6Xzyfy5wSNg+ceuW3btmnvnPSN5+To8iwu9mV8nFwuP3f+NPZkQWH+Zz7CZDB/P3F4w6aQpKRXfH7prVt/vk9969nRGyFkbGzi4ux24cKZjh7/zGfd0cMr8mJ4+/ZO5uYW2BfT9GlzTpw8goWi+w+iV6ya99OeFhE7tQJkoSb7+ut5lZXidd8vq6qqGjM6aHXIxvz83NVrFn23dkt9HzEwMNi0Yce+/TuwSOPg4PjtnCVDAkZir/r4+J2NOOnp6YM99PDodP7CmbFj/j3OEzRhqqOjy5nw3168eGpgYOjh3mn5ctiFrSnwnJZ+0aJF2piFDn+XNmZROzanBU3HmBovLM2VDAi2oroQXQDnyAGgFjguBIBa4Bw5ANQC58gBoBbIQgCoBbIQAGqBLASAWiALAaAWyEIAqAWyEABqgSwEgFogCwGgFshCyMiEVddlprqMyaDrtaTTagkFWQjRaEjErzazaUGXT5eXythcaCF8QBZCrZ30hfxqqqsglbRSYdm6BX1lEAqyEOo+xCzmSiHVVZCnIL1KUCJt72lAdSE6ArIQYrHpQSvsL+3PEgu0bxXaVBmvRfH3S0fPa011IboDz6tWtRq/QBZzrbQkV2rfwaCqQkHCElUIKZVKBp2kTKJUqvLTqxw6Gg6Ei1VxhWcL6cDcCRVlcn6+rLpaScKyioqKTpw4sWLFChKWhRDiGDCs7NiwIw53MI/cfxiZMo1MSfoKYGaUlFQlOXkZkrM4QBDIQgCoBY4LAaAWOC4EgFrguBAAaoEsBIBaIAsBoBbIQgCoBbIQAGqBLASAWiALAaAWyEIAqAWyEABqgSwEgFogCwGgFshCAKgFshAAaoEsBIBaIAsBoBY810IikQiyEGhp8GyhtWvXQhYCLQ2eLWRoaMhisXAcEADNh2cW+vHHH3EcDQCtgOdaiM/nw8SOjZeTk2NsbEx1FUBdeLZQenr6t99+i+OAukoqlW7atOngwYNhYWFU1wLUhWcLdenSZeLEic+fP8dxTN1z4sQJf39/b2/vU6dOwVpIB8Cc2uR5+PDh9u3bBw0atGjRIqprAbjB/2SCzMzMo0ePbt68GfeRtVd2dnZoaCiTyTx69GirVq2oLgfgiZC10B9//MFgML766ivcR9ZGYWFhDx8+XLVqVY8ePaiuBeAPNuQIFBkZGRoaumjRouDgYKprAUQh6lYZIpHozJkzBA2u+V68eBEUFJSSkvL48WPoH91G4Fro9OnTRUVFS5cuJWh8zVRWVrZ9+3Y+n79q1SonJyeqywGEI3ZDrqCgwNTUlM1uKXfGPXTo0Pnz50NCQgYMGEB1LYAkxN7zzNLSMiMjg9BFaIhbt24NGDCATqf/9ddf0D8tCrFXyDEYjJSUlLNnz65fv57QBVHo3bt3oaGh1tbWFy5cgEOlLRAZe+Tu3r3r4uLSurWu3WVaJpOFhoa+efMmJCTEy8uL6nIANWCndjOdPn16//79q1atCgwMpLoWQCWS7v8cExOjM+crPH78eNSoUUVFRTExMdA/gKTZQnr27Pn27dsnT55069aNnCUSITc3NzQ0FCH0yy+/6N52KWge2JBrrN27d9+7d2/VqlVffPEF1bUADULShhwmMzPz5MmTNQ8HDRpE5tKb7dKlSz179rSysrp8+TL0D/gIqdO+tW3btqCgICIiIjw8PDMzU09P7+bNm4MHDyazhiZ59erV9u3b3d3d7927p6enR3U5QBORPXPiypUr/f39BQIBjUaTyWTZ2dkkF1CnzMzM+fPnS6XSv/76C3tGIBCEhoYWFhZu2LDBxcWF6gKB5iK7hbp27apUKmsepqamklxAnUJDQ/Pz82ti4ZEjR8LDw0NCQrRlUxNQiLwWGjFiRHZ29kczBhcVFZFWQH3Cw8MTExNpNBqNRuvVq5e+vv64ceOio6OprgtoB/J2J1y9enXw4MFmZmYKhQJ7RqVSCYVC0gqoU0FBwenTpysrK7GHEokkIiJizpw51FYFtAipe+S2b9/+448/+vn5mZiYKJVKOp0uk8kKCwvJrOEjW7duzc3Nrf2MJu/eABqI1BZCCPn6+h4+fHjFihVOTk76+voymYzCbblz584lJCTQ6f/+ElQqlVwuh1OtQePhdmi1WqrMSqkqK6oWlculEqVErGjwI0XFRcVFxR4eHrgU0EgGPCZCyNCEYWzO2rh9aYUkn8lkYgmturrawMDAyMiIwWAcOHCAzKqA9sKhhV49EL6NE/ILpGZ2PJUKMdkMFodFo2noSQ80Gq1aIq+WKlRKlYhfqVLKbRwZ7j30W7Ux4nK5VFcHtI9aLfTiriD2RollOxOuMcfAjINrYSSRVcqFxWJhocimHbvfOAuOAYPqioCWaWYLlRXLo04WIRrT2tmMzqARUBjZynIrClP5XQaY+w2Ay+ZAEzSnhdISxbfDi9r72THZZO+NIFr+2xILa/rAYEuqCwFao8ktlJcmvR1ebO9jQ1hJFCvNEpqYKv2/Mqe6EKAdmtZCaYniR9fK7b11fErb0kwhmyUdPlPHf0yAiyZsiYnK5dFni3S+fxBC5m15VVWMJ1F8qgsBWqAJLXTzZFF7vzZEFqNBLB3Ncj7Ic95XUV0I0HSNbaGX98oUNCZDTxd2vjUS18LowcUSqqsAmq6xLRRztdSyfctK2Po8PRWdmfpKRHUhQKM1qoXiostbOZvRNHUNFJ94e8X33UTiMtxHtnAwS3hcgfuwQJc0qoXev6jQN9HKkw/UxOYy+QVSIV9OdSFAczXcQlUihZBfzTVuKVPLf8TQgvshAbblQL0avmo1+12VlQOB57w8e3Ht72cX8wtTbaydvD0HfNkjiEajIYROnl2LEK2zV8DZyE1SaWXbNp7DBi9o26Yj9qlrUfviXl1n63F9Og22srAnrjyepWFRNrQQqFfDa6HyYplcTtRp1y9e3Tx7cbOdrevaZReHDJz7ICb88vXd/1RGZ2ZmJz6Pv7H4299+XH+fydILj9yEvRTz9ELM0/Njhq1cPOe4uantX3ePEVQeQoipx8hPh13boF4Nt5CoXM5gETXFwtPnl9u39RkzYpWRoZlze9/B/rMfPzlXIfrnmKZUWjlh9Dpzs9YMBrNzp8HFJZlSaSVC6NHfEZ08/Dt17M/l8vw6D3dq70tQeQghJptRJYIsBOrViCwkVrI4hLSQUqlMz0pwcf53imDn9r4qlTI9Ix57aGXZjs3+5xoeDscIIVRZJVSpVCX8bGsrh5pP2dm6EVEehs6g6ekzZBINvfwJUK7h3lAqVCqGssG3NYNcLlMoqqNuH4y6fbD28xXif9ZCNFodHS6RipVKRU1rIYT09PSJKK+GtFJBh8uIQD0abiFDEyaf3/BV3M2gp8dh63G7eA/t5NG/9vPmZp+b8Z3DNqDTGdXVkppnpLJKIsrDKORKOoPGZGnqQTFAtca0EKOogJAWQgjZ2rhUSSqc2nfBHsrl1aVluSbG1p/5CI1GMzWxychK7PP/s1snpzwmqDyEkFyq0DeEdRCoV8NZyNyGTacRsiGHEBo6cG5S8v0nz68olcr0zPhTEd8dOj5fLpd9/lNeHQckvrkbn3gbIXTn4YnMnCSCykMIyarkNg7EbigCrdZwC7XrwC3KIGrCRIe23kvnnkjPiN+wPeDQbwurJKIZk3awWA0cxh3QZ0a3LqMuXd+14vtuySmPRw5Zgs1fRUSFomJRG+eWeGYGaKRGXXIXuT9Pz5hnaN4Sv4zf3s+csb4dm6trl7gDvDTqL8O9q2GVUEp8MRqnslza1s0A+gd8RqMO+Lj58WL+zDBuZainX/f7E17fjbi0pc6XuPq8yqq6twO7dRk1ImBRU6r9nPTM+GOnltf5klKpoNHotLpONe/dc+KgfrPqG7MorTRgMkxFAj6nsXMnvHtZ8Sxa1NrDqs5XpbIqcT3XGkilVWx23VuAenpcQwOTplTbAH5ZXlM/wmEbcrm8Ol+qKK5UiCsC59riURrQWU2YfuTPXwsZhjyOUUu52Vvx+6JBEy2MLVlUFwI0WhO28od9bZ0el6dUtIhTXfJeF3buawT9AxrUtKA8aXXbtCc5hBWjKfLeFDt5chw7GVJdCNACTZ6KUVKp+H1LpmNXOyZbN4/Z578t7tzb0LUL9A9olOZMCFwlUpzalmXjamVooVPHHKur5LmvC/0Gmnh0r3sHAwCfav6dHW7/UZybJrFwMDMw1fpGUipURR9KK/lVI76xsbRroZe4g+ZR6+YoBZmSB5ElKjqLwWbxLA30uGTfP1xNKqVKWFQpKhVXlkm6DTHr1Avu6QCaDIdbdOWlSd7Hiz4kigyM2dIqJVOPwWSzENLQHXd0Fr26slohU9DoSFBU1cbVwLWzgUtnI6rrAtoKtxtFIoTKi6oryuVioVwiVsokRF0foSY2h87Uo3N5DAMe06oNbLMBdeHZQgC0QHACJQBqgRYCQC3QQgCoBVoIALVACwGgFmghANTyf5NUt5/FgjaWAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}